from typing import List, Dict, Optional
from datetime import datetime
from loguru import logger
import sys
from pathlib import Path

sys.path.append('..')
from database.supabase_client import SupabaseClient
from database.models import PublishedArticle

class ArticleGenerator:
    """Claude Codeë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê¸€ ìƒì„± (í”„ë¡¬í”„íŠ¸ ë°©ì‹)"""

    def __init__(self, db_client: SupabaseClient):
        self.db = db_client
        self.prompts_dir = Path("prompts")
        self.prompts_dir.mkdir(exist_ok=True)
        logger.info("Article generator initialized with Claude Code mode")

    def generate_article(self, symbol: str, max_news: int = 5) -> Optional[str]:
        """íŠ¹ì • ì¢…ëª©ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ ì‘ì„± í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            # í•´ë‹¹ ì¢…ëª© ê´€ë ¨ ë¯¸ë°œí–‰ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            news_items = self.db.get_unpublished_news_by_symbol(symbol, limit=max_news)

            if not news_items:
                logger.info(f"No unpublished news found for {symbol}")
                return None

            logger.info(f"ğŸ“ Generating article prompt for {symbol} with {len(news_items)} news items")

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_article_prompt(symbol, news_items)

            # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            prompt_file = f"{self.prompts_dir}/article_{symbol}_{timestamp}.md"

            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(prompt)

            logger.info(f"âœ… Article prompt saved: {prompt_file}")
            logger.info(f"   â†’ Claude Codeì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³µì‚¬í•˜ì—¬ ê¸€ì„ ì‘ì„±í•˜ê³ ")
            logger.info(f"   â†’ ì™„ì„±ëœ ê¸€ì„ articles/ í´ë”ì— ì €ì¥í•˜ì„¸ìš”")

            return None  # ìˆ˜ë™ ì‘ì„±ì´ë¯€ë¡œ None ë°˜í™˜

        except Exception as e:
            logger.error(f"Error generating article prompt for {symbol}: {e}")

        return None

    def load_article_from_file(self, article_file: str, news_items: List[Dict]) -> Optional[str]:
        """Claude Codeì—ì„œ ì‘ì„±í•œ ê¸€ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # ì œëª©ê³¼ ë³¸ë¬¸ ë¶„ë¦¬
            article_data = self._parse_article_response(content)

            if not article_data:
                return None

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            analyzed_news_ids = [news['id'] for news in news_items]
            published_article = PublishedArticle(
                title=article_data['title'],
                content=article_data['content'],
                analyzed_news_ids=analyzed_news_ids,
                published_at=datetime.now()
            )

            article_id = self.db.insert_published_article(published_article)

            if article_id:
                logger.info(f"âœ… Article saved: {article_data['title']}")
                return article_id

        except Exception as e:
            logger.error(f"Error loading article: {e}")

        return None

    def _build_article_prompt(self, symbol: str, news_items: List[Dict]) -> str:
        """ë¸”ë¡œê·¸ ê¸€ ì‘ì„± í”„ë¡¬í”„íŠ¸"""
        # ë‰´ìŠ¤ ìš”ì•½ êµ¬ì„±
        news_summaries = []
        for i, news in enumerate(news_items, 1):
            raw_news = news['news_raw']
            analysis = news['analysis']

            summary = f"""
ë‰´ìŠ¤ {i}:
- ì œëª©: {raw_news['title']}
- ì¶œì²˜: {raw_news['source']}
- ë°œí–‰ì¼: {raw_news['published_at']}
- ë‚´ìš©: {raw_news['content'][:500]}
- ê´€ë ¨ì„± ì ìˆ˜: {news['relevance_score']}/100
- ì£¼ê°€ ì˜í–¥: {news['price_impact']}
- ë¶„ì„: {analysis.get('reasoning', '')}
- í•µì‹¬ í¬ì¸íŠ¸:
{chr(10).join([f"  * {point}" for point in analysis.get('key_points', [])])}
"""
            news_summaries.append(summary)

        news_context = "\n".join(news_summaries)

        return f"""ë‹¹ì‹ ì€ ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ ì „ë¬¸ ë¸”ë¡œê±°ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ê¸€ì„ ì‘ì„±í•˜ë©°, íˆ¬ììë“¤ì—ê²Œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì¢…ëª©: {symbol}

ê´€ë ¨ ë‰´ìŠ¤:
{news_context}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

---
ì œëª©: [ì¢…ëª©ëª…] ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ë¶„ì„ - [í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œ]

## ğŸ“Š ë¬´ì—‡ì´ ì¼ì–´ë‚¬ëŠ”ê°€

[3-5ê°œ ë‰´ìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬ í•µì‹¬ ì‚¬ê±´ ìš”ì•½. ê° ë‰´ìŠ¤ì˜ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ.]

## ğŸ”„ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€

[ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…: ì´ ë‰´ìŠ¤ë“¤ì´ ë¹„ì¦ˆë‹ˆìŠ¤/ì‚°ì—…ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€]
- ê³µê¸‰ë§ ë³€í™”
- ì‹œì¥ ì ìœ ìœ¨ ë³€í™”
- ê·œì œ ì˜í–¥
- ê²½ìŸì‚¬ ê´€ê³„
ë“±ì„ ê³ ë ¤

## ğŸ’¡ ì™œ ì£¼ê°€ì— ì˜í–¥ì„ ì£¼ëŠ”ê°€

[ë…¼ë¦¬ì  ì—°ê²°ê³ ë¦¬ ì„¤ëª…]
1. ì¬ë¬´ì  ì˜í–¥: ë§¤ì¶œ/ì´ìµì— ì–´ë–»ê²Œ ì˜í–¥?
2. ì‹œì¥ ì‹¬ë¦¬: íˆ¬ììë“¤ì´ ì–´ë–»ê²Œ ë°˜ì‘í• ì§€?
3. ì¥ë‹¨ê¸° ì „ë§: ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸°ì  ê´€ì 

## ğŸ“ˆ íˆ¬ì ì‹œì‚¬ì 

íˆ¬ìì ê´€ì ì—ì„œ ê³ ë ¤ì‚¬í•­:

**ê¸ì •ì  ìš”ì†Œ**
- [êµ¬ì²´ì  ê¸ì • ìš”ì¸ 3ê°€ì§€]

**ë¦¬ìŠ¤í¬ ìš”ì†Œ**
- [êµ¬ì²´ì  ë¦¬ìŠ¤í¬ 3ê°€ì§€]

**íˆ¬ì ì „ëµ ì œì•ˆ**
- [ë‹¨ê¸° íˆ¬ììë¥¼ ìœ„í•œ ê´€ì ]
- [ì¥ê¸° íˆ¬ììë¥¼ ìœ„í•œ ê´€ì ]

## ğŸ”— ì°¸ê³  ìë£Œ

[ì›ë³¸ ë‰´ìŠ¤ ì¶œì²˜ ë§í¬ë“¤ì„ ì •ë¦¬]

---

**ì‘ì„± ê°€ì´ë“œë¼ì¸:**
1. ê¸€ì ìˆ˜: 1,500-2,000ì (SEO ìµœì í™”)
2. í†¤: ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ
3. ë°ì´í„°: êµ¬ì²´ì ì¸ ìˆ«ìì™€ íŒ©íŠ¸ ì¤‘ì‹¬
4. ê°ê´€ì„±: ê³¼ì¥ ì—†ì´ ê· í˜•ì¡íŒ ì‹œê°
5. SEO: ì¢…ëª©ëª…ê³¼ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
6. íˆ¬ì ì¡°ì–¸ ì•„ë‹˜: "íˆ¬ì íŒë‹¨ì€ ë³¸ì¸ì˜ ì±…ì„" ëª…ì‹œ

ì œëª©ê³¼ ë³¸ë¬¸ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±:

TITLE:
[ì œëª©]

CONTENT:
[ë³¸ë¬¸ ì „ì²´]
"""

    def _parse_article_response(self, response_text: str) -> Optional[Dict]:
        """Claude ì‘ë‹µì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ ë¶„ë¦¬"""
        try:
            # TITLE: ê³¼ CONTENT: ë¡œ êµ¬ë¶„
            title_start = response_text.find("TITLE:")
            content_start = response_text.find("CONTENT:")

            if title_start == -1 or content_start == -1:
                # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ì²« ì¤„ì„ ì œëª©ìœ¼ë¡œ
                lines = response_text.strip().split('\n')
                title = lines[0].strip()
                content = '\n'.join(lines[1:]).strip()
            else:
                title = response_text[title_start + 6:content_start].strip()
                content = response_text[content_start + 8:].strip()

            # ì œëª©ì—ì„œ ë§ˆí¬ë‹¤ìš´ ì œê±°
            title = title.replace('#', '').strip()

            return {
                'title': title,
                'content': content
            }

        except Exception as e:
            logger.error(f"Error parsing article response: {e}")
            return None

    def generate_daily_articles(self, tier: str = "tier_1", criteria_override: Dict = None) -> List[str]:
        """
        ë¶„ì„ ê¸°ì¤€ì ì— ë”°ë¼ ì—¬ëŸ¬ ê¸€ ìƒì„± í”„ë¡¬í”„íŠ¸

        Args:
            tier: "tier_1" (ë†’ì€ ì¤‘ìš”ë„ 3ê°œ+), "tier_2" (ë‰´ìŠ¤ 3ê°œ+ AND ì ìˆ˜ 75+), "tier_3" (ìƒìœ„ 15ê°œ)
            criteria_override: ê¸°ì¤€ì  ì‚¬ìš©ì ì •ì˜ (ì„ íƒì‚¬í•­)
        """
        article_ids = []

        try:
            from config.settings import (
                ARTICLE_TIER_1_SYMBOLS, ARTICLE_TIER_2_SYMBOLS,
                ARTICLE_TIER_1_MIN_HIGH_IMPORTANCE,
                ARTICLE_TIER_2_MIN_NEWS, ARTICLE_TIER_2_MIN_SCORE,
                ARTICLE_TIER_3_TOP_N
            )
            from analyzers.analysis_pipeline import AnalysisPipeline

            pipeline = AnalysisPipeline(self.db)

            # ê¸°ì¤€ì ë³„ ì¢…ëª© ì„ íƒ
            if tier == "tier_1":
                logger.info(f"ğŸ¯ Generating articles for Tier 1 (High Importance >= {ARTICLE_TIER_1_MIN_HIGH_IMPORTANCE})")
                target_symbols = ARTICLE_TIER_1_SYMBOLS
                logger.info(f"   Target symbols ({len(target_symbols)}): {', '.join(target_symbols)}")

            elif tier == "tier_2":
                logger.info(f"ğŸ¯ Generating articles for Tier 2 (News >= {ARTICLE_TIER_2_MIN_NEWS} AND Score >= {ARTICLE_TIER_2_MIN_SCORE})")
                target_symbols = ARTICLE_TIER_1_SYMBOLS + ARTICLE_TIER_2_SYMBOLS
                logger.info(f"   Target symbols ({len(target_symbols)}): {', '.join(target_symbols)}")

            elif tier == "tier_3":
                logger.info(f"ğŸ¯ Generating articles for Tier 3 (Top {ARTICLE_TIER_3_TOP_N} symbols)")
                trending_symbols = pipeline.get_trending_symbols()
                target_symbols = list(trending_symbols.keys())[:ARTICLE_TIER_3_TOP_N]
                logger.info(f"   Target symbols ({len(target_symbols)}): {', '.join(target_symbols)}")

            else:
                raise ValueError(f"Invalid tier: {tier}")

            # ê¸€ ìƒì„±
            logger.info(f"\nğŸ“ Starting article generation for {len(target_symbols)} symbols...")
            for i, symbol in enumerate(target_symbols, 1):
                try:
                    result = self.generate_article(symbol)
                    logger.info(f"   [{i}/{len(target_symbols)}] {symbol}: {'Generated' if result else 'No data'}")
                    if result:
                        article_ids.append(result)
                except Exception as e:
                    logger.error(f"   [{i}/{len(target_symbols)}] {symbol}: Error - {str(e)[:50]}")

            logger.info(f"\nâœ… Article generation completed: {len(article_ids)} articles generated")
            logger.info(f"   Tier: {tier}")
            logger.info(f"   Target symbols: {len(target_symbols)}")
            logger.info(f"   Generated: {len(article_ids)}")

        except Exception as e:
            logger.error(f"Error in daily article generation: {e}")

        return article_ids

    def export_for_wordpress(self, article_id: str) -> Dict:
        """WordPress ë°œí–‰ìš© ë°ì´í„° í¬ë§·"""
        try:
            articles = self.db.get_recent_articles(days=1, limit=100)
            article = next((a for a in articles if a['id'] == article_id), None)

            if not article:
                return None

            # WordPress XML-RPC í¬ë§·
            wordpress_data = {
                'title': article['title'],
                'content': article['content'],
                'post_status': 'publish',
                'post_type': 'post',
                'comment_status': 'open',
                'categories': ['Stock Analysis', 'US Market'],
                'tags': self._extract_tags(article),
                'custom_fields': [
                    {'key': 'article_id', 'value': article_id},
                    {'key': 'generated_at', 'value': article['created_at']}
                ]
            }

            return wordpress_data

        except Exception as e:
            logger.error(f"Error exporting for WordPress: {e}")
            return None

    def _extract_tags(self, article: Dict) -> List[str]:
        """ê¸€ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        # analyzed_news_idsë¥¼ í†µí•´ ê´€ë ¨ ì¢…ëª© ì¶”ì¶œ
        # ê°„ë‹¨íˆ ì œëª©ì—ì„œ ì¶”ì¶œ
        tags = ['Stock News', 'US Market', 'Investment']
        return tags
