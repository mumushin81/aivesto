from typing import List, Dict, Optional
from datetime import datetime
from loguru import logger
import sys
from pathlib import Path

sys.path.append('..')
from database.supabase_client import SupabaseClient
from database.models import PublishedArticle

class ArticleGenerator:
    """Claude Codeë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ê¸€ ìƒì„± (í”„ë¡¬í”„íŠ¸ ë°©ì‹)"""

    def __init__(self, db_client: SupabaseClient):
        self.db = db_client
        self.prompts_dir = Path("prompts")
        self.prompts_dir.mkdir(exist_ok=True)
        logger.info("Article generator initialized with Claude Code mode")

    def generate_article(self, symbol: str, max_news: int = 5) -> Optional[str]:
        """íŠ¹ì • ì¢…ëª©ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ ì‘ì„± í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            # í•´ë‹¹ ì¢…ëª© ê´€ë ¨ ë¯¸ë°œí–‰ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            news_items = self.db.get_unpublished_news_by_symbol(symbol, limit=max_news)

            if not news_items:
                logger.info(f"No unpublished news found for {symbol}")
                return None

            logger.info(f"ğŸ“ Generating article prompt for {symbol} with {len(news_items)} news items")

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_article_prompt(symbol, news_items)

            # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            prompt_file = f"{self.prompts_dir}/article_{symbol}_{timestamp}.md"

            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(prompt)

            logger.info(f"âœ… Article prompt saved: {prompt_file}")
            logger.info(f"   â†’ Claude Codeì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³µì‚¬í•˜ì—¬ ê¸€ì„ ì‘ì„±í•˜ê³ ")
            logger.info(f"   â†’ ì™„ì„±ëœ ê¸€ì„ articles/ í´ë”ì— ì €ì¥í•˜ì„¸ìš”")

            return None  # ìˆ˜ë™ ì‘ì„±ì´ë¯€ë¡œ None ë°˜í™˜

        except Exception as e:
            logger.error(f"Error generating article prompt for {symbol}: {e}")

        return None

    def load_article_from_file(self, article_file: str, news_items: List[Dict]) -> Optional[str]:
        """Claude Codeì—ì„œ ì‘ì„±í•œ ê¸€ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # ì œëª©ê³¼ ë³¸ë¬¸ ë¶„ë¦¬
            article_data = self._parse_article_response(content)

            if not article_data:
                return None

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            analyzed_news_ids = [news['id'] for news in news_items]
            published_article = PublishedArticle(
                title=article_data['title'],
                content=article_data['content'],
                analyzed_news_ids=analyzed_news_ids,
                published_at=datetime.now()
            )

            article_id = self.db.insert_published_article(published_article)

            if article_id:
                logger.info(f"âœ… Article saved: {article_data['title']}")
                return article_id

        except Exception as e:
            logger.error(f"Error loading article: {e}")

        return None

    def _build_article_prompt(self, symbol: str, news_items: List[Dict]) -> str:
        """SEO ìµœì í™” ê¸€ ì‘ì„± í”„ë¡¬í”„íŠ¸ (êµ¬ê¸€ & AI ê²€ìƒ‰ ë…¸ì¶œ ìµœìš°ì„ )"""
        # ë‰´ìŠ¤ ìš”ì•½ êµ¬ì„± (ìƒì„¸ ì •ë³´ í¬í•¨)
        news_summaries = []
        for i, news in enumerate(news_items, 1):
            raw_news = news['news_raw']
            analysis = news['analysis']
            published_date = raw_news['published_at'].split('T')[0] if 'T' in raw_news['published_at'] else raw_news['published_at']

            summary = f"""
[ë‰´ìŠ¤ {i}]
ì œëª©: {raw_news['title']}
ì¶œì²˜: {raw_news['source']}
ë°œí–‰ì¼: {published_date}
ë‚´ìš©: {raw_news['content'][:600]}

ë¶„ì„ ê²°ê³¼:
- ê´€ë ¨ì„± ì ìˆ˜: {news['relevance_score']}/100
- ì£¼ê°€ ì˜í–¥: {news['price_impact']}
- ì¤‘ìš”ë„: {analysis.get('importance', 'medium')}
- ë¶„ì„: {analysis.get('reasoning', '')}
- í•µì‹¬ í¬ì¸íŠ¸:
  {chr(10).join([f"â€¢ {point}" for point in analysis.get('key_points', [])])}
"""
            news_summaries.append(summary)

        news_context = "\n".join(news_summaries)

        return f"""ë‹¹ì‹ ì€ ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ êµ¬ê¸€ ë° AI ê²€ìƒ‰ ì—”ì§„(Claude, ChatGPT, Gemini)ì— ë…¸ì¶œë˜ê¸° ì¢‹ì€ ê¸€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ğŸ“Š ì‘ì„± ëŒ€ìƒ: {symbol}

ë¶„ì„ ëŒ€ìƒ ë‰´ìŠ¤:
{news_context}

================================================================================
ğŸ¯ ê¸€ ì‘ì„± ê°€ì´ë“œ (SEO ìµœì í™”)
================================================================================

## âœ… í•„ìˆ˜ êµ¬ì„± (ì´ ìˆœì„œëŒ€ë¡œ)

### 1ï¸âƒ£ ì œëª© (60ì ì´ë‚´, ê²€ìƒ‰ í´ë¦­ë¥  ê·¹ëŒ€í™”)
- ê·œì¹™: ì¢…ëª©ëª… + í•µì‹¬ ì´ìŠˆ + ìˆ˜ì¹˜ ë˜ëŠ” ì§ˆë¬¸
- ì•½í•¨: "{symbol} ìµœì‹  ë‰´ìŠ¤"
- ê°•í•¨: "{symbol} ë¹„ì¦ˆë‹ˆìŠ¤ ì „í™˜ì  + êµ¬ì²´ì  ìˆ˜ì¹˜ (ì‹œê°„)"

### 2ï¸âƒ£ 10ì´ˆ íŒë… ìš”ì•½ (AI ê²€ìƒ‰ ìµœìš°ì„ , í•„ìˆ˜)
ë‹¤ìŒ ì •í™•í•œ í˜•ì‹ìœ¼ë¡œ ì‘ì„±:

## ğŸ“Œ í•µì‹¬ ìš”ì•½ (AI ê²€ìƒ‰ ì—”ì§„ìš©)

**ìƒí™©**: 1ì¤„ - ë°©ê¸ˆ ìˆì€ êµ¬ì²´ì  ë‰´ìŠ¤ ì´ë²¤íŠ¸
**ì˜í–¥**: 1-2ì¤„ - ì‹œì¥ì´ë‚˜ ì‹¤ì ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (ìˆ˜ì¹˜ í¬í•¨)
**íˆ¬ìì ê´€ì **:
- ê¸ì •: [êµ¬ì²´ì  ì´ìœ  1-2ê°œ]
- ìœ„í—˜: [êµ¬ì²´ì  ì´ìœ  1-2ê°œ]
- ì‹œì‚¬ì : [êµ¬ì²´ì  ì•¡ì…˜]

### 3ï¸âƒ£ ğŸ“Š ë¬´ì—‡ì´ ì¼ì–´ë‚¬ëŠ”ê°€ (ë‰´ìŠ¤)
- ëª…í™•í•œ í—¤ë“œë¼ì¸ (í•œ ë¬¸ì¥)
- ì–¸ì œ: ì •í™•í•œ ë‚ ì§œ ëª…ì‹œ
- ëˆ„ê°€: íšŒì‚¬ëª…, ê²½ì˜ì§„
- ë­: êµ¬ì²´ì  ì•¡ì…˜ê³¼ ìˆ˜ì¹˜
- ì¶œì²˜: ê³µì‹ ë°œí‘œ, ë§í¬ ëª…ì‹œ

### 4ï¸âƒ£ ğŸ”§ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€ (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)
- ì´ ë‰´ìŠ¤ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë¯¸ì¹˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…
- ì—…ê³„ ë°°ê²½ì§€ì‹ (ì‹ ê·œ ë…ì ê³ ë ¤)
- ê²½ìŸ êµ¬ë„ ë³€í™” (ë¹„êµ í¬í•¨)
- ì‹œì¥ íŠ¸ë Œë“œì™€ì˜ ì—°ê²°

### 5ï¸âƒ£ ğŸ’° ì™œ ì£¼ê°€ì— ì˜í–¥ì„ ì£¼ëŠ”ê°€ (íˆ¬ì ë¡œì§)
1. ì¬ë¬´ ì„íŒ©íŠ¸
   - ìˆ˜ìµì„± ê°œì„ : [êµ¬ì²´ì  ë©”ì»¤ë‹ˆì¦˜]
   - ì„±ì¥ì„±: [êµ¬ì²´ì  ë°ì´í„°]
   - ì‹¤ì œ ìˆ˜ì¹˜: [ì •í™•í•œ ìˆ«ì]

2. íˆ¬ìì ì‹¬ë¦¬
   - ì‹œì¥ ì„ í˜¸ë„: [ê¸°ê´€íˆ¬ìì ì›€ì§ì„]
   - ë¶„ì„ê°€ í‰ê°€: [ì „ë¬¸ê°€ ì˜ê²¬]
   - ë°¸ë¥˜ì—ì´ì…˜: [P/E, PEG ë“±]

3. ë‹¨ê¸° vs ì¥ê¸° ì „ë§
   - 3-6ê°œì›”: [ì‹ í˜¸]
   - 6-12ê°œì›”: [ê¸°ëŒ€]
   - 1-3ë…„: [ì¥ê¸° ì¶”ì„¸]

### 6ï¸âƒ£ ğŸ“ˆ ìˆ˜ì¹˜ë¡œ ë³´ëŠ” ë¶„ì„ (í‘œ í˜•ì‹)
ë§ˆí¬ë‹¤ìš´ í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµ ì •ë³´ ì œì‹œ:
- {symbol} vs ê²½ìŸì‚¬ ë¹„êµ
- ì‹¤ì  ì¶”ì´ (3ë…„)
- ìœ„í—˜ ìš”ì†Œ í‰ê°€

### 7ï¸âƒ£ ğŸ¢ ê²½ìŸì‚¬ ë¹„êµ (ì°¨ë³„ì„±)
- vs ì£¼ìš” ê²½ìŸì‚¬: [ê°•ì  ë° ì•½ì ]
- ì‹œì¥ ìœ„ì¹˜: [{symbol}ì˜ ì°¨ë³„ì„±]
- ê²°ë¡ : [ê°ê´€ì  í‰ê°€]

### 8ï¸âƒ£ â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ - ë°˜ë“œì‹œ í¬í•¨)
Q&A í˜•ì‹ìœ¼ë¡œ 5-7ê°œ ì§ˆë¬¸ì— ë‹µë³€:
- Q: "{symbol}ì„ ì§€ê¸ˆ ì‚¬ì•¼ í•˜ë‚˜?"
- Q: "{symbol} vs ê²½ìŸì‚¬ ë¹„êµ"
- Q: "ì´ ì‚°ì—…ì€ ì •ë§ ì„±ì¥í• ê¹Œ?"
- Q: "{symbol}ì˜ ìœ„í—˜ì€?"
- Q: "ì£¼ê°€ ì „ë§ì€?"

ê° ë‹µë³€ì€ 150-300ì, ëª…í™•í•˜ê³  ê·¼ê±° ìˆê²Œ.

### 9ï¸âƒ£ ğŸ“° ì „ë¬¸ê°€ ì˜ê²¬ ë° ì¶œì²˜ (ì‹ ë¢°ë„)
- ì£¼ìš” ë¶„ì„ê°€ ë“±ê¸‰ (Goldman Sachs, Morgan Stanley ë“±)
- ëª©í‘œê°€ ë° ê·¼ê±°
- ê³µì‹ ê³µì‹œ ë° íˆ¬ìì ë¬¸ì„œ ë§í¬
- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ì›

### ğŸ”Ÿ âš¡ ë‹¨ê³„ë³„ ì „ë§ ë¶„ì„
ë‹¨ê¸°(3-6ê°œì›”), ì¤‘ê¸°(6-12ê°œì›”), ì¥ê¸°(1-3ë…„)ë³„ë¡œ:
- ê°•ì„¸/ì¤‘ë¦½/ì•½ì„¸ ì‹œë‚˜ë¦¬ì˜¤
- ê° ì‹œë‚˜ë¦¬ì˜¤ì˜ í™•ë¥ 
- íŠ¸ë¦¬ê±°ì™€ ëª©í‘œê°€
- ìµœì•…ì˜ ê²½ìš°ê¹Œì§€ ê³ ë ¤

### 1ï¸âƒ£1ï¸âƒ£ ğŸ” ê²°ë¡  ë° íˆ¬ì ê³ ë ¤ì‚¬í•­
- âœ… ì¢‹ì€ ì´ìœ  (ì²´í¬ë¦¬ìŠ¤íŠ¸)
- âš ï¸ ìœ„í—˜ ìš”ì†Œ (ì²´í¬ë¦¬ìŠ¤íŠ¸)
- ğŸ“‹ íˆ¬ì ê²°ì • ê°€ì´ë“œ (íˆ¬ììë³„)
- âš–ï¸ ë²•ì  ê³ ì§€ (íˆ¬ì ì¡°ì–¸ ì•„ë‹˜ ëª…ì‹œ)

================================================================================
ğŸ”‘ SEO í‚¤ì›Œë“œ ë°°ì¹˜ ê·œì¹™
================================================================================

1ì°¨ í‚¤ì›Œë“œ (ì¢…ëª©ëª…): ì œëª© 1íšŒ, ì²« 100ì 1íšŒ, ë³¸ë¬¸ 1íšŒ ì´ìƒ
2ì°¨ í‚¤ì›Œë“œ (ì£¼ì œ): ìì—°ìŠ¤ëŸ½ê²Œ 3-5íšŒ ë¶„ì‚° ë°°ì¹˜
3ì°¨ í‚¤ì›Œë“œ (Long-tail): "~í•˜ëŠ” ì´ìœ ", "~ì „ë§", "vs ë¹„êµ" í¬í•¨
4ì°¨ í‚¤ì›Œë“œ (ì˜ë„): "ë§¤ìˆ˜ ê¸°íšŒ", "íˆ¬ì ì‹œì ", "ìœ„í—˜ ìš”ì†Œ" ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨

ê·œì¹™: í‚¤ì›Œë“œë¥¼ ê°•ì œë¡œ ë°˜ë³µí•˜ì§€ ë§ê³ , ë¬¸ë§¥ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì´ê¸°

================================================================================
âœï¸ ì‘ì„± ìŠ¤íƒ€ì¼
================================================================================

- ê¸€ì ìˆ˜: 2,500-3,500ì (ê¸°ì¡´ 1,500-2,000ì—ì„œ ì¦ê°€ - ê¹Šì´ ì¶”ê°€)
- í†¤: ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ (í•™ìˆ ì  ì–¸ì–´ ì œê±°)
- ë‹¨ë½: 200ì ì´ë‚´ (AIê°€ íŒŒì‹±í•˜ê¸° ì‰½ê²Œ)
- êµ¬ì¡°: H2(##), H3(###)ë¡œ ê³„ì¸µí™”
- ë¦¬ìŠ¤íŠ¸: ë¶ˆë¦¿(â€¢) ë˜ëŠ” ìˆ«ìë¡œ ëª…í™•í™”
- í‘œ: ë°ì´í„° ë¹„êµëŠ” ë§ˆí¬ë‹¤ìš´ í‘œ ì‚¬ìš©
- ë§í¬: ëª¨ë“  ì¶œì²˜ì— ëª…í™•í•œ ë§í¬ í¬í•¨
- ìˆ˜ì¹˜: ì¶”ì •ì¹˜(~)ê°€ ì•„ë‹Œ ì •í™•í•œ ìˆ«ì ì‚¬ìš©
- ë©´ì±…: "íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤" ë°˜ë“œì‹œ í¬í•¨

================================================================================
ğŸ“‹ ìµœì¢… í˜•ì‹
================================================================================

TITLE:
[í´ë¦­ ìœ ë„ì ì¸ ì œëª© 60ì ì´ë‚´]

CONTENT:
[ì „ì²´ ë³¸ë¬¸ - ìœ„ì˜ í•„ìˆ˜ êµ¬ì„± 1-11 í¬í•¨, ë§ˆí¬ë‹¤ìš´ í˜•ì‹]

================================================================================
âš ï¸ ê¸ˆì§€ ì‚¬í•­
================================================================================

âŒ í•˜ì§€ ë§ ê²ƒ:
- "~ì— íˆ¬ìí•˜ì„¸ìš”", "ì§€ê¸ˆ ì‚¬ì„¸ìš”" ê°™ì€ ì§ì ‘ì  ê¶Œê³ 
- ê³¼ì¥ëœ ìˆ˜ì¹˜ ("í™•ì‹¤íˆ 2ë°° ì˜¬ë¼ê°ˆ", "ë¬´ì¡°ê±´ ì„±ê³µ")
- ì¶œì²˜ ì—†ëŠ” ì£¼ì¥
- ë„ˆë¬´ ê¸´ ë‹¨ë½ (200ì ì´ˆê³¼)
- ë‹¨ì¡°ë¡œìš´ ë¬¸ì¥ (í‘œ, ë¦¬ìŠ¤íŠ¸, ê°•ì¡°ë¡œ ì‹œê°í™”)
- ì •ì¹˜, ì¢…êµ, ìœ¤ë¦¬ ë…¼ìŸ (íˆ¬ì ê¸€ì—ë§Œ ì§‘ì¤‘)

================================================================================

ì§€ê¸ˆ ì‹œì‘í•˜ì„¸ìš”. ê¸€ì„ ì‘ì„±í•œ í›„ ìœ„ì˜ í˜•ì‹ (TITLE: / CONTENT:)ë¡œ ê²°ê³¼ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

ì°¸ê³ : ì´ ê¸€ì€ êµ¬ê¸€, Claude, ChatGPT, Gemini ë“± ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ì—ì„œ ìµœìƒìœ„ì—
ë…¸ì¶œë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ë¶€ë¶„ì˜ ì—­í• ì„ ì´í•´í•˜ê³  ì‹ ì¤‘í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""

    def _parse_article_response(self, response_text: str) -> Optional[Dict]:
        """Claude ì‘ë‹µì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ ë¶„ë¦¬"""
        try:
            # TITLE: ê³¼ CONTENT: ë¡œ êµ¬ë¶„
            title_start = response_text.find("TITLE:")
            content_start = response_text.find("CONTENT:")

            if title_start == -1 or content_start == -1:
                # êµ¬ë¶„ìê°€ ì—†ìœ¼ë©´ ì²« ì¤„ì„ ì œëª©ìœ¼ë¡œ
                lines = response_text.strip().split('\n')
                title = lines[0].strip()
                content = '\n'.join(lines[1:]).strip()
            else:
                title = response_text[title_start + 6:content_start].strip()
                content = response_text[content_start + 8:].strip()

            # ì œëª©ì—ì„œ ë§ˆí¬ë‹¤ìš´ ì œê±°
            title = title.replace('#', '').strip()

            return {
                'title': title,
                'content': content
            }

        except Exception as e:
            logger.error(f"Error parsing article response: {e}")
            return None

    def generate_daily_articles(self, tier: str = "tier_1", criteria_override: Dict = None) -> List[str]:
        """
        ë¶„ì„ ê¸°ì¤€ì ì— ë”°ë¼ ì—¬ëŸ¬ ê¸€ ìƒì„± í”„ë¡¬í”„íŠ¸

        Args:
            tier: "tier_1" (ë†’ì€ ì¤‘ìš”ë„ 3ê°œ+), "tier_2" (ë‰´ìŠ¤ 3ê°œ+ AND ì ìˆ˜ 75+), "tier_3" (ìƒìœ„ 15ê°œ)
            criteria_override: ê¸°ì¤€ì  ì‚¬ìš©ì ì •ì˜ (ì„ íƒì‚¬í•­)
        """
        article_ids = []

        try:
            from config.settings import (
                ARTICLE_TIER_1_SYMBOLS, ARTICLE_TIER_2_SYMBOLS,
                ARTICLE_TIER_1_MIN_HIGH_IMPORTANCE,
                ARTICLE_TIER_2_MIN_NEWS, ARTICLE_TIER_2_MIN_SCORE,
                ARTICLE_TIER_3_TOP_N
            )
            from analyzers.analysis_pipeline import AnalysisPipeline

            pipeline = AnalysisPipeline(self.db)

            # ê¸°ì¤€ì ë³„ ì¢…ëª© ì„ íƒ
            if tier == "tier_1":
                logger.info(f"ğŸ¯ Generating articles for Tier 1 (High Importance >= {ARTICLE_TIER_1_MIN_HIGH_IMPORTANCE})")
                target_symbols = ARTICLE_TIER_1_SYMBOLS
                logger.info(f"   Target symbols ({len(target_symbols)}): {', '.join(target_symbols)}")

            elif tier == "tier_2":
                logger.info(f"ğŸ¯ Generating articles for Tier 2 (News >= {ARTICLE_TIER_2_MIN_NEWS} AND Score >= {ARTICLE_TIER_2_MIN_SCORE})")
                target_symbols = ARTICLE_TIER_1_SYMBOLS + ARTICLE_TIER_2_SYMBOLS
                logger.info(f"   Target symbols ({len(target_symbols)}): {', '.join(target_symbols)}")

            elif tier == "tier_3":
                logger.info(f"ğŸ¯ Generating articles for Tier 3 (Top {ARTICLE_TIER_3_TOP_N} symbols)")
                trending_symbols = pipeline.get_trending_symbols()
                target_symbols = list(trending_symbols.keys())[:ARTICLE_TIER_3_TOP_N]
                logger.info(f"   Target symbols ({len(target_symbols)}): {', '.join(target_symbols)}")

            else:
                raise ValueError(f"Invalid tier: {tier}")

            # ê¸€ ìƒì„±
            logger.info(f"\nğŸ“ Starting article generation for {len(target_symbols)} symbols...")
            for i, symbol in enumerate(target_symbols, 1):
                try:
                    result = self.generate_article(symbol)
                    logger.info(f"   [{i}/{len(target_symbols)}] {symbol}: {'Generated' if result else 'No data'}")
                    if result:
                        article_ids.append(result)
                except Exception as e:
                    logger.error(f"   [{i}/{len(target_symbols)}] {symbol}: Error - {str(e)[:50]}")

            logger.info(f"\nâœ… Article generation completed: {len(article_ids)} articles generated")
            logger.info(f"   Tier: {tier}")
            logger.info(f"   Target symbols: {len(target_symbols)}")
            logger.info(f"   Generated: {len(article_ids)}")

        except Exception as e:
            logger.error(f"Error in daily article generation: {e}")

        return article_ids

    def export_for_wordpress(self, article_id: str) -> Dict:
        """WordPress ë°œí–‰ìš© ë°ì´í„° í¬ë§·"""
        try:
            articles = self.db.get_recent_articles(days=1, limit=100)
            article = next((a for a in articles if a['id'] == article_id), None)

            if not article:
                return None

            # WordPress XML-RPC í¬ë§·
            wordpress_data = {
                'title': article['title'],
                'content': article['content'],
                'post_status': 'publish',
                'post_type': 'post',
                'comment_status': 'open',
                'categories': ['Stock Analysis', 'US Market'],
                'tags': self._extract_tags(article),
                'custom_fields': [
                    {'key': 'article_id', 'value': article_id},
                    {'key': 'generated_at', 'value': article['created_at']}
                ]
            }

            return wordpress_data

        except Exception as e:
            logger.error(f"Error exporting for WordPress: {e}")
            return None

    def _extract_tags(self, article: Dict) -> List[str]:
        """ê¸€ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        # analyzed_news_idsë¥¼ í†µí•´ ê´€ë ¨ ì¢…ëª© ì¶”ì¶œ
        # ê°„ë‹¨íˆ ì œëª©ì—ì„œ ì¶”ì¶œ
        tags = ['Stock News', 'US Market', 'Investment']
        return tags
