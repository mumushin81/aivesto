from typing import Dict, List, Tuple
import json
from loguru import logger
import sys
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

sys.path.append('..')
from config.settings import MIN_RELEVANCE_SCORE, ANTHROPIC_API_KEY
from database.models import AnalyzedNews, PriceImpact, Importance

try:
    from anthropic import Anthropic
    CLAUDE_AVAILABLE = True
except ImportError:
    CLAUDE_AVAILABLE = False
    logger.warning("Anthropic API not available - falling back to prompt generation mode")

class RelevanceAnalyzer:
    """ìë™ ë‰´ìŠ¤ ë¶„ì„ ë° íˆ¬ì ì‹œê·¸ë„ ë¶„ë¥˜ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.prompts_dir = Path("prompts")
        self.prompts_dir.mkdir(exist_ok=True)

        # Claude API ì´ˆê¸°í™”
        if CLAUDE_AVAILABLE and ANTHROPIC_API_KEY:
            self.client = Anthropic(api_key=ANTHROPIC_API_KEY)
            self.auto_analyze = True
            logger.info("Relevance analyzer initialized with Claude API (automatic mode)")
        else:
            self.client = None
            self.auto_analyze = False
            logger.info("Relevance analyzer initialized with prompt generation mode (manual analysis)")

    def analyze_news(self, news_data: Dict) -> Dict:
        """ë‰´ìŠ¤ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìˆ˜ë™ ë¶„ì„ìš©)"""
        try:
            title = news_data.get('title', '')
            content = news_data.get('content', '')
            existing_symbols = news_data.get('symbols', [])
            news_id = news_data.get('id', 'unknown')

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_analysis_prompt(title, content, existing_symbols)

            # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì €ì¥
            prompt_file = self._save_prompt(news_id, prompt)
            logger.info(f"ğŸ“ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±: {prompt_file}")
            logger.info(f"   ë‰´ìŠ¤: {title[:60]}")
            logger.info(f"   â†’ Claude Codeì—ì„œ ë‹¤ìŒì„ ì‹¤í–‰í•˜ì„¸ìš”:")
            logger.info(f"   cat {prompt_file}")

            return None  # ìˆ˜ë™ ë¶„ì„ì´ë¯€ë¡œ None ë°˜í™˜

        except Exception as e:
            logger.error(f"Error generating analysis prompt: {e}")
            return None

    def load_manual_analysis(self, json_response: str) -> Dict:
        """Claude Codeì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ë¶„ì„í•œ JSON ê²°ê³¼ ë¡œë“œ"""
        try:
            result = self._parse_response(json_response)
            return result
        except Exception as e:
            logger.error(f"Error loading manual analysis: {e}")
            return None

    def _save_prompt(self, news_id: str, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.prompts_dir}/analysis_{news_id}_{timestamp}.md"

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(prompt)

        return filename

    def _build_analysis_prompt(self, title: str, content: str, existing_symbols: List[str]) -> str:
        """Claudeì—ê²Œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        return f"""ë‹¹ì‹ ì€ ë¯¸êµ­ ì£¼ì‹ ì‹œì¥ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì£¼ì‹ íˆ¬ììì—ê²Œ ì–¼ë§ˆë‚˜ ìœ ìš©í•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ì œëª©: {title}

ë‰´ìŠ¤ ë‚´ìš©:
{content[:2000]}

ê¸°ì¡´ ì¶”ì¶œëœ ì‹¬ë³¼: {', '.join(existing_symbols) if existing_symbols else 'ì—†ìŒ'}

ë‹¤ìŒ í•­ëª©ì„ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

1. relevance_score (0-100): ì£¼ì‹ íˆ¬ììì—ê²Œ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€
   - 0-30: ë¬´ê´€í•œ ë‰´ìŠ¤ (ì¼ë°˜ ë‰´ìŠ¤, ì •ì¹˜, ìŠ¤í¬ì¸  ë“±)
   - 31-60: ê°„ì ‘ ê´€ë ¨ (ê²½ì œ ì¼ë°˜, ì—…ê³„ íŠ¸ë Œë“œ)
   - 61-80: ì§ì ‘ ê´€ë ¨ (íŠ¹ì • ê¸°ì—…/ì„¹í„° ë‰´ìŠ¤)
   - 81-100: ë§¤ìš° ì¤‘ìš” (ì‹¤ì , M&A, ê·œì œ ë³€í™”, ì¤‘ëŒ€ ì‚¬ê±´)

2. affected_symbols: ì˜í–¥ì„ ë°›ëŠ” ì£¼ì‹ ì‹¬ë³¼ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 5ê°œ)
   - ê¸°ì¡´ ì‹¬ë³¼ ê²€ì¦ ë° ì¶”ê°€ ì‹¬ë³¼ ë°œê²¬
   - ì§ì ‘ ì–¸ê¸‰ëœ ê¸°ì—…ë§Œ í¬í•¨

3. price_impact: ì£¼ê°€ ì˜í–¥ ì˜ˆì¸¡
   - "up": ê¸ì •ì  ì˜í–¥ (ë§¤ì¶œ ì¦ê°€, ì‹ ì œí’ˆ, í˜¸ì‹¤ì )
   - "down": ë¶€ì •ì  ì˜í–¥ (ì†ì‹¤, ì†Œì†¡, ê·œì œ)
   - "neutral": ì¤‘ë¦½ ë˜ëŠ” í˜¼ì¬

4. importance: ì¤‘ìš”ë„
   - "high": ì¦‰ê°ì  ì£¼ê°€ ì˜í–¥ ì˜ˆìƒ
   - "medium": ì¤‘ê¸°ì  ì˜í–¥
   - "low": ì¥ê¸°ì /ê°„ì ‘ì  ì˜í–¥

5. reasoning: ë¶„ì„ ê·¼ê±° (2-3ë¬¸ì¥)
   - ì™œ ì´ ì ìˆ˜ì¸ì§€
   - ì£¼ê°€ì— ì–´ë–¤ ì˜í–¥ì„ ì¤„ì§€
   - í•µì‹¬ íŒ©íŠ¸

6. key_points: í•µì‹¬ í¬ì¸íŠ¸ (3-5ê°œ bullet points)
   - íˆ¬ììê°€ ì•Œì•„ì•¼ í•  í•µì‹¬ ì •ë³´

ì‘ë‹µ í˜•ì‹ (JSONë§Œ ë°˜í™˜):
{{
  "relevance_score": 85,
  "affected_symbols": ["AAPL", "MSFT"],
  "price_impact": "up",
  "importance": "high",
  "reasoning": "...",
  "key_points": ["...", "...", "..."]
}}"""

    def _parse_response(self, response_text: str) -> Dict:
        """Claude Code ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ ì‹œë„
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                data = json.loads(json_str)

                # ìœ íš¨ì„± ê²€ì¦
                required_fields = ['relevance_score', 'affected_symbols', 'price_impact', 'importance']
                if not all(field in data for field in required_fields):
                    logger.error("Missing required fields in response")
                    return None

                # ê´€ë ¨ì„± ì ìˆ˜ê°€ ì„ê³„ê°’ ì´í•˜ë©´ í•„í„°ë§
                if int(data['relevance_score']) < MIN_RELEVANCE_SCORE:
                    logger.info(f"Filtered out (score: {data['relevance_score']})")
                    return None

                # íƒ€ì… ë³€í™˜
                result = {
                    'relevance_score': int(data['relevance_score']),
                    'affected_symbols': data['affected_symbols'],
                    'price_impact': PriceImpact(data['price_impact']),
                    'importance': Importance(data['importance']),
                    'reasoning': data.get('reasoning', ''),
                    'key_points': data.get('key_points', [])
                }

                logger.info(f"âœ… Analysis loaded: score {result['relevance_score']}")
                return result

        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
        except Exception as e:
            logger.error(f"Response parsing error: {e}")

        return None

    def batch_analyze(self, news_list: List[Dict], batch_size: int = 10, max_workers: int = 5) -> List[Dict]:
        """ì—¬ëŸ¬ ë‰´ìŠ¤ ìë™ ë¶„ì„ (Claude API í™œìš©)"""
        if not self.auto_analyze:
            logger.info(f"ğŸ“ Generating analysis prompts for {len(news_list)} news items (manual mode)...")
            for news in news_list:
                self.analyze_news(news)
            logger.info(f"âœ… All prompts generated in prompts/ directory")
            return []

        logger.info(f"ğŸ¤– Starting automated analysis for {len(news_list)} news items...")
        results = []

        try:
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_news = {
                    executor.submit(self._analyze_single_news_auto, news): news
                    for news in news_list
                }

                completed = 0
                for future in as_completed(future_to_news):
                    try:
                        result = future.result()
                        if result:
                            results.append(result)
                            completed += 1
                    except Exception as e:
                        logger.error(f"Error analyzing news: {e}")

                logger.info(f"âœ… Automated analysis completed: {completed}/{len(news_list)} items analyzed")

        except Exception as e:
            logger.error(f"Batch analysis error: {e}")

        return results

    def _analyze_single_news_auto(self, news_data: Dict) -> Dict:
        """ê°œë³„ ë‰´ìŠ¤ ìë™ ë¶„ì„"""
        try:
            title = news_data.get('title', '')
            content = news_data.get('content', '')
            existing_symbols = news_data.get('symbols', [])
            news_id = news_data.get('id', 'unknown')

            prompt = self._build_analysis_prompt(title, content, existing_symbols)

            # Claude APIë¡œ ë¶„ì„
            message = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=500,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            response_text = message.content[0].text

            # ë¶„ì„ ê²°ê³¼ íŒŒì‹±
            result = self._parse_response(response_text)
            if result:
                result['news_id'] = news_id
                # ì‹ í˜¸ ë ˆë²¨ ê³„ì‚°
                result['signal_level'] = self._calculate_signal_level(result)
                return result

            return None

        except Exception as e:
            logger.error(f"Error analyzing news {news_data.get('id')}: {e}")
            return None

    def _calculate_signal_level(self, analysis_result: Dict) -> int:
        """íˆ¬ì ì‹œê·¸ë„ ë ˆë²¨ ê³„ì‚° (1-4)
        Level 1: ë§¤ìš° ì¤‘ìš” (90+ì ) - ì¦‰ì‹œ ì‹¤í–‰
        Level 2: ë†’ìŒ (80-89ì ) - ê³ ë ¤ í•„ìš”
        Level 3: ì¤‘ê°„ (70-79ì ) - ëª¨ë‹ˆí„°ë§
        Level 4: ë‚®ìŒ (70ì  ë¯¸ë§Œ) - ì°¸ê³ ìš©
        """
        score = analysis_result.get('relevance_score', 0)
        importance = analysis_result.get('importance', 'low')

        # ì¤‘ìš”ë„ì™€ ì ìˆ˜ ì¡°í•©ìœ¼ë¡œ ë ˆë²¨ ê²°ì •
        if score >= 90 or (score >= 85 and importance == 'high'):
            return 1
        elif score >= 80 or (score >= 75 and importance == 'high'):
            return 2
        elif score >= 70:
            return 3
        else:
            return 4
