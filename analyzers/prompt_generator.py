"""
Claude Codeë¥¼ í™œìš©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
API ì—†ì´ ì‚¬ìš©ìê°€ Claude Codeì—ì„œ ì§ì ‘ ë¶„ì„/ì‘ì„±í•  ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ ìƒì„±
"""

from typing import List, Dict
from datetime import datetime
from loguru import logger
import sys

sys.path.append('..')
from database.supabase_client import SupabaseClient

class PromptGenerator:
    """ë‰´ìŠ¤ ë¶„ì„ ë° ê¸€ ì‘ì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„±"""

    def __init__(self, db_client: SupabaseClient):
        self.db = db_client
        logger.info("Prompt generator initialized")

    def generate_analysis_prompt(self, news_batch: List[Dict], output_file: str = None) -> str:
        """ë‰´ìŠ¤ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        prompt = f"""# ë¯¸êµ­ ì£¼ì‹ ë‰´ìŠ¤ ë¶„ì„ ì‘ì—…

ì´ {len(news_batch)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

## ë¶„ì„ ë°©ë²•

ê° ë‰´ìŠ¤ì— ëŒ€í•´ ë‹¤ìŒì„ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„:

1. **relevance_score** (0-100): ì£¼ì‹ íˆ¬ìì ê´€ë ¨ì„±
   - 0-30: ë¬´ê´€ (ì¼ë°˜ ë‰´ìŠ¤, ì •ì¹˜, ìŠ¤í¬ì¸ )
   - 31-60: ê°„ì ‘ ê´€ë ¨ (ê²½ì œ ì¼ë°˜)
   - 61-80: ì§ì ‘ ê´€ë ¨ (íŠ¹ì • ê¸°ì—…/ì„¹í„°)
   - 81-100: ë§¤ìš° ì¤‘ìš” (ì‹¤ì , M&A, ê·œì œ)

2. **affected_symbols**: ì˜í–¥ë°›ëŠ” ì£¼ì‹ ì‹¬ë³¼ (ìµœëŒ€ 5ê°œ)

3. **price_impact**: "up", "down", "neutral"

4. **importance**: "high", "medium", "low"

5. **reasoning**: ë¶„ì„ ê·¼ê±° (2-3ë¬¸ì¥)

6. **key_points**: í•µì‹¬ í¬ì¸íŠ¸ (3-5ê°œ)

---

"""

        for i, news in enumerate(news_batch, 1):
            prompt += f"""
## ë‰´ìŠ¤ #{i}

**ID**: {news['id']}
**ì¶œì²˜**: {news['source']}
**ì œëª©**: {news['title']}
**ë°œí–‰ì¼**: {news['published_at']}

**ë‚´ìš©**:
{news.get('content', '')[:1000]}

**URL**: {news['url']}

---

"""

        prompt += """
## ì¶œë ¥ í˜•ì‹

ê° ë‰´ìŠ¤ì— ëŒ€í•´ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±:

```json
{
  "news_id": "uuid",
  "relevance_score": 85,
  "affected_symbols": ["AAPL", "MSFT"],
  "price_impact": "up",
  "importance": "high",
  "reasoning": "ì• í”Œì´ ì‹ ì œí’ˆì„ ë°œí‘œí•˜ì—¬ ë§¤ì¶œ ì¦ê°€ ì˜ˆìƒ. ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ì™€ì˜ íŒŒíŠ¸ë„ˆì‹­ë„ ê¸ì •ì .",
  "key_points": [
    "ì‹ ì œí’ˆ ë°œí‘œë¡œ ë§¤ì¶œ ì¦ê°€ ì „ë§",
    "ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€ ê°€ëŠ¥ì„±",
    "MSì™€ì˜ í˜‘ë ¥ìœ¼ë¡œ ì‹œë„ˆì§€ ê¸°ëŒ€"
  ]
}
```

**ê´€ë ¨ì„± ì ìˆ˜ 70 ì´ìƒì¸ ë‰´ìŠ¤ë§Œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.**

ë¶„ì„ ê²°ê³¼ë¥¼ `analysis_results.json` íŒŒì¼ì— JSON ë°°ì—´ë¡œ ì €ì¥í•˜ì„¸ìš”.
"""

        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(prompt)
            logger.info(f"Analysis prompt saved to {output_file}")

        return prompt

    def generate_article_prompt(self, symbol: str, news_items: List[Dict], output_file: str = None) -> str:
        """ë¸”ë¡œê·¸ ê¸€ ì‘ì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        prompt = f"""# {symbol} ì¢…ëª© ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ ì‘ì„±

{len(news_items)}ê°œì˜ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬ í•œêµ­ì–´ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

---

"""

        for i, news in enumerate(news_items, 1):
            raw_news = news['news_raw']
            analysis = news.get('analysis', {})

            prompt += f"""
## ë‰´ìŠ¤ #{i}

**ì œëª©**: {raw_news['title']}
**ì¶œì²˜**: {raw_news['source']}
**ë°œí–‰ì¼**: {raw_news['published_at']}

**ë‚´ìš©**:
{raw_news.get('content', '')[:800]}

**ê´€ë ¨ì„± ì ìˆ˜**: {news.get('relevance_score', 'N/A')}/100
**ì£¼ê°€ ì˜í–¥**: {news.get('price_impact', 'N/A')}
**ì¤‘ìš”ë„**: {news.get('importance', 'N/A')}

**ë¶„ì„**:
{analysis.get('reasoning', '')}

**í•µì‹¬ í¬ì¸íŠ¸**:
{chr(10).join([f'- {point}' for point in analysis.get('key_points', [])])}

**ì¶œì²˜**: {raw_news['url']}

---

"""

        prompt += f"""
## ì‘ì„± ê°€ì´ë“œ

ë‹¤ìŒ êµ¬ì¡°ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”:

### ì œëª© í˜•ì‹
`[{symbol}] ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ë¶„ì„ - [í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œ]`

ì˜ˆ: "AAPL ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ë¶„ì„ - ì‹ ì œí’ˆ ì¶œì‹œ, ì‹¤ì  í˜¸ì¡°, AI íˆ¬ì"

### ë³¸ë¬¸ êµ¬ì¡°

```markdown
# [ì œëª©]

## ğŸ“Š ë¬´ì—‡ì´ ì¼ì–´ë‚¬ëŠ”ê°€

[3-5ê°œ ë‰´ìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬ í•µì‹¬ ì‚¬ê±´ ìš”ì•½. ê° ë‰´ìŠ¤ì˜ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ.]

## ğŸ”„ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€

[ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…: ì´ ë‰´ìŠ¤ë“¤ì´ ë¹„ì¦ˆë‹ˆìŠ¤/ì‚°ì—…ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€]

- ê³µê¸‰ë§ ë³€í™”
- ì‹œì¥ ì ìœ ìœ¨ ë³€í™”
- ê·œì œ ì˜í–¥
- ê²½ìŸì‚¬ ê´€ê³„

ë“±ì„ ê³ ë ¤í•˜ì—¬ ì„¤ëª…

## ğŸ’¡ ì™œ ì£¼ê°€ì— ì˜í–¥ì„ ì£¼ëŠ”ê°€

[ë…¼ë¦¬ì  ì—°ê²°ê³ ë¦¬ ì„¤ëª…]

**1. ì¬ë¬´ì  ì˜í–¥**
- ë§¤ì¶œ/ì´ìµì— ì–´ë–»ê²Œ ì˜í–¥?

**2. ì‹œì¥ ì‹¬ë¦¬**
- íˆ¬ììë“¤ì´ ì–´ë–»ê²Œ ë°˜ì‘í• ì§€?

**3. ì¥ë‹¨ê¸° ì „ë§**
- ë‹¨ê¸°/ì¤‘ê¸°/ì¥ê¸°ì  ê´€ì 

## ğŸ“ˆ íˆ¬ì ì‹œì‚¬ì 

íˆ¬ìì ê´€ì ì—ì„œ ê³ ë ¤ì‚¬í•­:

**ê¸ì •ì  ìš”ì†Œ**
- [êµ¬ì²´ì  ê¸ì • ìš”ì¸ 3ê°€ì§€]

**ë¦¬ìŠ¤í¬ ìš”ì†Œ**
- [êµ¬ì²´ì  ë¦¬ìŠ¤í¬ 3ê°€ì§€]

**íˆ¬ì ì „ëµ ì œì•ˆ**
- ë‹¨ê¸° íˆ¬ìì: [ê´€ì ]
- ì¥ê¸° íˆ¬ìì: [ê´€ì ]

## ğŸ”— ì°¸ê³  ìë£Œ

[ì›ë³¸ ë‰´ìŠ¤ ì¶œì²˜ ë§í¬ë“¤ì„ ì •ë¦¬]

---

**ë©´ì±… ì¡°í•­**: ë³¸ ê¸€ì€ ì •ë³´ ì œê³µ ëª©ì ì´ë©° íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹™ë‹ˆë‹¤. íˆ¬ì ê²°ì •ì€ ë³¸ì¸ì˜ ì±…ì„ í•˜ì— ì‹ ì¤‘íˆ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
```

### ì‘ì„± ìš”êµ¬ì‚¬í•­

1. **ê¸€ì ìˆ˜**: 1,500-2,000ì (SEO ìµœì í™”)
2. **í†¤**: ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ
3. **ë°ì´í„°**: êµ¬ì²´ì ì¸ ìˆ«ìì™€ íŒ©íŠ¸ ì¤‘ì‹¬
4. **ê°ê´€ì„±**: ê³¼ì¥ ì—†ì´ ê· í˜•ì¡íŒ ì‹œê°
5. **SEO**: {symbol} ì¢…ëª©ëª…ê³¼ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
6. **í•œêµ­ì–´**: íˆ¬ìì ì¹œí™”ì  ë¬¸ì²´

ì‘ì„±í•œ ê¸€ì„ `article_{symbol}_{datetime.now().strftime('%Y%m%d_%H%M')}.md` íŒŒì¼ë¡œ ì €ì¥í•˜ì„¸ìš”.
"""

        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(prompt)
            logger.info(f"Article prompt saved to {output_file}")

        return prompt

    def generate_daily_workflow_prompt(self, output_dir: str = "prompts") -> str:
        """ì¼ì¼ ì›Œí¬í”Œë¡œìš° í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        # ë¯¸ë¶„ì„ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        unanalyzed_news = self.db.get_unanalyzed_news(limit=50)

        if not unanalyzed_news:
            logger.info("No unanalyzed news found")
            return None

        timestamp = datetime.now().strftime('%Y%m%d_%H%M')

        # 1. ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        analysis_prompt_file = f"{output_dir}/analysis_{timestamp}.md"
        self.generate_analysis_prompt(unanalyzed_news, analysis_prompt_file)

        # ì›Œí¬í”Œë¡œìš° ê°€ì´ë“œ
        workflow_prompt = f"""# ì¼ì¼ ë‰´ìŠ¤ ë¶„ì„ ë° ë¸”ë¡œê·¸ ê¸€ ì‘ì„± ì›Œí¬í”Œë¡œìš°

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ”„ ì‘ì—… ìˆœì„œ

### 1ë‹¨ê³„: ë‰´ìŠ¤ ë¶„ì„ ({len(unanalyzed_news)}ê°œ)

í”„ë¡¬í”„íŠ¸ íŒŒì¼: `{analysis_prompt_file}`

**ì‘ì—…**:
1. ìœ„ íŒŒì¼ì„ ì—´ì–´ì„œ ë‰´ìŠ¤ ë¶„ì„ í”„ë¡¬í”„íŠ¸ í™•ì¸
2. Claude Codeì—ì„œ ê° ë‰´ìŠ¤ ë¶„ì„
3. ë¶„ì„ ê²°ê³¼ë¥¼ `{output_dir}/analysis_results_{timestamp}.json`ì— ì €ì¥

**ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥**:
```bash
python scripts/save_analysis.py {output_dir}/analysis_results_{timestamp}.json
```

---

### 2ë‹¨ê³„: ì¸ê¸° ì¢…ëª© í™•ì¸

ë¶„ì„ ì™„ë£Œ í›„, ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¡œ íŠ¸ë Œë”© ì¢…ëª© í™•ì¸:

```bash
python scripts/get_trending.py
```

---

### 3ë‹¨ê³„: ë¸”ë¡œê·¸ ê¸€ ì‘ì„±

íŠ¸ë Œë”© ì¢…ëª©(ì˜ˆ: AAPL, TSLA, NVDA ë“±)ì— ëŒ€í•´ ê¸€ ì‘ì„± í”„ë¡¬í”„íŠ¸ ìƒì„±:

```bash
python scripts/generate_article_prompts.py AAPL TSLA NVDA
```

ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ íŒŒì¼ë“¤:
- `{output_dir}/article_AAPL_{timestamp}.md`
- `{output_dir}/article_TSLA_{timestamp}.md`
- `{output_dir}/article_NVDA_{timestamp}.md`

ê° í”„ë¡¬í”„íŠ¸ë¥¼ ì—´ì–´ì„œ ë¸”ë¡œê·¸ ê¸€ ì‘ì„± í›„, ê²°ê³¼ë¥¼ ì§€ì •ëœ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥.

---

### 4ë‹¨ê³„: ê¸€ ë°œí–‰

ì‘ì„±í•œ ê¸€ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥:

```bash
python scripts/publish_articles.py articles/article_AAPL_{timestamp}.md
```

---

## ğŸ“ ìš”ì•½

**ì˜¤ëŠ˜ì˜ ì‘ì—…**:
- [ ] {len(unanalyzed_news)}ê°œ ë‰´ìŠ¤ ë¶„ì„
- [ ] ë¶„ì„ ê²°ê³¼ ì €ì¥
- [ ] íŠ¸ë Œë”© ì¢…ëª© í™•ì¸
- [ ] ìƒìœ„ 3-5ê°œ ì¢…ëª© ê¸€ ì‘ì„±
- [ ] ì‘ì„±í•œ ê¸€ ë°œí–‰

**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 30-60ë¶„

**ì™„ì „ ë¬´ë£Œ**: API ë¹„ìš© $0
"""

        workflow_file = f"{output_dir}/workflow_{timestamp}.md"
        with open(workflow_file, 'w', encoding='utf-8') as f:
            f.write(workflow_prompt)

        logger.info(f"Daily workflow prompt saved to {workflow_file}")
        return workflow_file
