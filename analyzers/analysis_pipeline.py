from typing import List, Dict
from loguru import logger
import sys

sys.path.append('..')
from database.supabase_client import SupabaseClient
from database.models import AnalyzedNews
from analyzers.relevance_analyzer import RelevanceAnalyzer

class AnalysisPipeline:
    """ë‰´ìŠ¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, db_client: SupabaseClient):
        self.db = db_client
        self.analyzer = RelevanceAnalyzer()
        logger.info("Analysis pipeline initialized")

    def run_analysis(self, limit: int = 50) -> int:
        """ë¯¸ë¶„ì„ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•˜ê³  ì €ì¥"""
        try:
            # ë¯¸ë¶„ì„ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            unanalyzed_news = self.db.get_unanalyzed_news(limit=limit)

            if not unanalyzed_news:
                logger.info("No unanalyzed news found")
                return 0

            logger.info(f"Found {len(unanalyzed_news)} unanalyzed news items")

            # ë°°ì¹˜ ë¶„ì„
            analysis_results = self.analyzer.batch_analyze(unanalyzed_news)

            # ì €ì¥
            saved_count = 0
            for result in analysis_results:
                analyzed_news = AnalyzedNews(
                    raw_news_id=result['news_id'],
                    relevance_score=result['relevance_score'],
                    affected_symbols=result['affected_symbols'],
                    price_impact=result['price_impact'],
                    importance=result['importance'],
                    signal_level=result.get('signal_level', 4),  # ì‹ í˜¸ ë ˆë²¨ í¬í•¨
                    analysis={
                        'reasoning': result.get('reasoning', ''),
                        'key_points': result.get('key_points', [])
                    }
                )

                news_id = self.db.insert_analyzed_news(analyzed_news)
                if news_id:
                    saved_count += 1
                    # ì‹ í˜¸ ë ˆë²¨ì— ë”°ë¼ ë¡œê¹…
                    signal_name = {1: "ğŸ”´ URGENT", 2: "ğŸŸ  HIGH", 3: "ğŸŸ¡ MEDIUM", 4: "ğŸŸ¢ LOW"}
                    logger.info(f"{signal_name.get(result.get('signal_level', 4), '?')} | {result['relevance_score']} points | {', '.join(result['affected_symbols'])}")

            logger.info(f"Analysis pipeline completed: {saved_count} news items analyzed and saved")
            return saved_count

        except Exception as e:
            logger.error(f"Analysis pipeline error: {e}")
            return 0

    def get_trending_symbols(self, hours: int = 24) -> Dict[str, int]:
        """ìµœê·¼ íŠ¸ë Œë”© ì¢…ëª© ë¶„ì„"""
        try:
            high_relevance_news = self.db.get_high_relevance_news(limit=100)

            symbol_counts = {}
            for news in high_relevance_news:
                for symbol in news['affected_symbols']:
                    symbol_counts[symbol] = symbol_counts.get(symbol, 0) + 1

            # ì •ë ¬
            trending = dict(sorted(symbol_counts.items(), key=lambda x: x[1], reverse=True))
            logger.info(f"Trending symbols: {list(trending.keys())[:10]}")

            return trending

        except Exception as e:
            logger.error(f"Error getting trending symbols: {e}")
            return {}
