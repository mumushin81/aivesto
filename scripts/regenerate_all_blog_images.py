#!/usr/bin/env python3
"""
ë¸”ë¡œê·¸ ì´ë¯¸ì§€ ì „ì²´ ì¬ìƒì„± ìŠ¤í¬ë¦½íŠ¸
1. ê¸°ì¡´ ì´ë¯¸ì§€ ì œê±° ì™„ë£Œ
2. AI í”„ë¡¬í”„íŠ¸ ë¡œë“œ
3. Midjourney ì´ë¯¸ì§€ ìƒì„± (auto_crop=True for 4 crops)
4. ìµœê³  í¬ë¡­ ì„ íƒ ë° ë‹¤ìš´ë¡œë“œ
5. ë¸”ë¡œê·¸ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì—…ë°ì´íŠ¸
"""
import os
import sys
import json
import requests
from pathlib import Path
from typing import Dict, List

# magic_book ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
magic_book_path = Path.home() / 'dev' / 'magic_book'
sys.path.insert(0, str(magic_book_path))

from dotenv import load_dotenv
from supabase import create_client
from loguru import logger
from src.midjourney import generate_images_batch_and_save

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(env_path)

supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_ANON_KEY")

if not supabase_url or not supabase_key:
    raise ValueError("SUPABASE_URL and SUPABASE_ANON_KEY must be set in .env file")

# ë¸”ë¡œê·¸ ì•„í‹°í´ë³„ ì´ë¯¸ì§€ ë§µí•‘
ARTICLE_IMAGE_MAPPING = {
    "article_NVDA_blackwell_gpu_20251113.md": [
        {"key": "NVDA_blackwell_chip", "filename": "NVDA_blackwell_chip_1.jpg", "caption": "NVIDIA Blackwell GPUê°€ AI ì„±ëŠ¥ì„ 5ë°° í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤"},
        {"key": "NVDA_ai_datacenter", "filename": "NVDA_datacenter_demand_2.jpg", "caption": "í´ë¼ìš°ë“œ 3ì‚¬ê°€ Blackwell GPUë¥¼ ëŒ€ëŸ‰ êµ¬ë§¤í•˜ê³  ìˆìŠµë‹ˆë‹¤"}
    ],
    "article_NVDA_foxconn_ai_server_20251115.md": [
        {"key": "NVDA_blackwell_chip", "filename": "NVDA_foxconn_factory_1.jpg", "caption": "í­ìŠ¤ì½˜ AI ì„œë²„ ìƒì‚° ë¼ì¸"},
        {"key": "NVDA_ai_datacenter", "filename": "NVDA_ai_server_tech_2.jpg", "caption": "ì°¨ì„¸ëŒ€ AI ì„œë²„ ì•„í‚¤í…ì²˜"}
    ],
    "article_TSLA_robotaxi_fleet_20251113.md": [
        {"key": "TSLA_robotaxi", "filename": "TSLA_robotaxi_autonomous_1.jpg", "caption": "Tesla ë¡œë³´íƒì‹œ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ"},
        {"key": "TSLA_charging_network", "filename": "TSLA_charging_network_2.jpg", "caption": "Tesla ìŠˆí¼ì°¨ì € ë„¤íŠ¸ì›Œí¬ í™•ì¥"}
    ],
    "article_AAPL_iphone_sales_20251113.md": [
        {"key": "AAPL_premium_iphone", "filename": "AAPL_iphone_premium_1.jpg", "caption": "í”„ë¦¬ë¯¸ì—„ iPhone ë¼ì¸ì—…"},
        {"key": "AAPL_enterprise", "filename": "AAPL_enterprise_2.jpg", "caption": "ê¸°ì—…ìš© Apple ìƒíƒœê³„"}
    ],
    "article_ADBE_creative_ai_20251113.md": [
        {"key": "ADBE_creative_cloud", "filename": "ADBE_creative_cloud_1.jpg", "caption": "Adobe Creative Cloud ì›Œí¬í”Œë¡œìš°"},
        {"key": "ADBE_firefly_ai", "filename": "ADBE_firefly_ai_2.jpg", "caption": "Adobe Firefly AI ìƒì„± ë„êµ¬"}
    ],
    "article_AMZN_aws_ai_services_20251113.md": [
        {"key": "AMZN_aws_cloud", "filename": "AMZN_aws_datacenter_1.jpg", "caption": "AWS í´ë¼ìš°ë“œ ì¸í”„ë¼"},
        {"key": "AMZN_ai_services", "filename": "AMZN_ai_platform_2.jpg", "caption": "AWS AI ì„œë¹„ìŠ¤ í”Œë«í¼"}
    ],
    "article_GOOGL_search_ai_20251113.md": [
        {"key": "GOOGL_search_ai", "filename": "GOOGL_search_ai_1.jpg", "caption": "Google AI ê²€ìƒ‰ í˜ì‹ "},
        {"key": "GOOGL_advertising", "filename": "GOOGL_ad_business_2.jpg", "caption": "Google ê´‘ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ì¥"}
    ],
    "article_META_enterprise_ai_20251113.md": [
        {"key": "META_business_ai", "filename": "META_business_tools_2.jpg", "caption": "Meta ë¹„ì¦ˆë‹ˆìŠ¤ AI ë„êµ¬"},
        {"key": "META_llama_opensource", "filename": "META_llama_opensource_1.jpg", "caption": "Meta Llama ì˜¤í”ˆì†ŒìŠ¤ AI"}
    ],
    "article_MSFT_AI_office_integration_20251113.md": [
        {"key": "MSFT_copilot", "filename": "MSFT_copilot_integration_1.jpg", "caption": "Microsoft Copilot í†µí•©"},
        {"key": "MSFT_copilot", "filename": "MSFT_revenue_growth_2.jpg", "caption": "Microsoft AI ìˆ˜ìµ ì„±ì¥"}
    ],
    "article_NFLX_subscriber_growth_20251113.md": [
        {"key": "NFLX_streaming", "filename": "NFLX_streaming_content_1.jpg", "caption": "Netflix ìŠ¤íŠ¸ë¦¬ë° ì½˜í…ì¸ "},
        {"key": "NFLX_advertising", "filename": "NFLX_advertising_2.jpg", "caption": "Netflix ê´‘ê³  í”Œë«í¼"}
    ],
    "article_UBER_profitability_expansion_20251113.md": [
        {"key": "UBER_rideshare", "filename": "UBER_rideshare_profit_1.jpg", "caption": "Uber ë¼ì´ë“œì…°ì–´ ìˆ˜ìµì„±"},
        {"key": "UBER_eats", "filename": "UBER_eats_delivery_2.jpg", "caption": "Uber Eats ë°°ë‹¬ ì„±ì¥"}
    ]
}

def load_prompts() -> Dict:
    """AI ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ë¡œë“œ"""
    prompt_file = Path(__file__).parent / 'ai_image_prompts.json'
    with open(prompt_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def download_image(url: str, save_path: Path):
    """URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    response = requests.get(url)
    response.raise_for_status()
    save_path.parent.mkdir(parents=True, exist_ok=True)
    with open(save_path, 'wb') as f:
        f.write(response.content)
    logger.success(f"  âœ“ Downloaded: {save_path.name}")

def get_best_crop_url(parent_image_id: str) -> str:
    """Supabaseì—ì„œ ìµœê³  ì¶”ì²œ í¬ë¡­ ì´ë¯¸ì§€ URL ì¡°íšŒ"""
    supabase = create_client(supabase_url, supabase_key)

    # ì›ë³¸ ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
    original = supabase.table('midjourney_images')\
        .select('*')\
        .eq('image_id', parent_image_id)\
        .single()\
        .execute()

    if not original.data or 'metadata' not in original.data:
        # ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´ ì²«ë²ˆì§¸ í¬ë¡­ ë°˜í™˜
        crop = supabase.table('midjourney_images')\
            .select('*')\
            .eq('parent_image_id', parent_image_id)\
            .eq('image_type', 'cropped')\
            .limit(1)\
            .execute()
        return crop.data[0]['public_url'] if crop.data else original.data['public_url']

    # AI ì¶”ì²œ ì •ë³´ ìˆìœ¼ë©´ ìµœê³  í¬ë¡­ ë°˜í™˜
    best_crop_position = original.data['metadata'].get('ai_recommendation', {}).get('best_crop', 'top_left')

    crop = supabase.table('midjourney_images')\
        .select('*')\
        .eq('parent_image_id', parent_image_id)\
        .eq('crop_position', best_crop_position)\
        .single()\
        .execute()

    return crop.data['public_url'] if crop.data else original.data['public_url']

def update_markdown_images(article_filename: str, image_mapping: List[Dict]):
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì˜ ì´ë¯¸ì§€ ê²½ë¡œ ì—…ë°ì´íŠ¸"""
    article_path = Path(__file__).parent.parent / 'articles' / article_filename

    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # ì´ë¯¸ì§€ ì°¸ì¡° ì—…ë°ì´íŠ¸
    for img in image_mapping:
        # ê¸°ì¡´ íŒ¨í„´ ì°¾ê¸°: ![caption](../images/filename.jpg)
        old_pattern = f"](../images/{img['filename']})"
        if old_pattern in content:
            logger.info(f"  âœ“ Image reference found for {img['filename']}")
        else:
            logger.warning(f"  âš ï¸  Image reference not found for {img['filename']}, will be added")

    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(content)

    logger.success(f"âœ… Updated: {article_filename}")

def main():
    logger.info("=" * 80)
    logger.info("ğŸ”„ ë¸”ë¡œê·¸ ì´ë¯¸ì§€ ì „ì²´ ì¬ìƒì„±")
    logger.info("=" * 80)

    # 1. AI í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    logger.info("\nğŸ“ Step 1: Loading AI image prompts...")
    prompts_data = load_prompts()
    logger.success(f"  âœ“ Loaded {len(prompts_data)} prompt templates")

    # 2. í•„ìš”í•œ ê³ ìœ  ì´ë¯¸ì§€ í‚¤ ì¶”ì¶œ
    unique_keys = set()
    for article, images in ARTICLE_IMAGE_MAPPING.items():
        for img in images:
            unique_keys.add(img['key'])

    logger.info(f"\nğŸ¨ Step 2: Generating {len(unique_keys)} unique Midjourney images...")
    logger.info(f"  Keys: {', '.join(unique_keys)}")

    # 3. Midjourney í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    midjourney_prompts = []
    key_to_prompt = {}
    for key in unique_keys:
        if key in prompts_data:
            prompt = prompts_data[key]['midjourney_prompt']
            midjourney_prompts.append(prompt)
            key_to_prompt[key] = prompt
            logger.info(f"  â€¢ {key}: {prompt[:60]}...")
        else:
            logger.warning(f"  âš ï¸  No prompt found for key: {key}")

    # 4. Midjourney ì´ë¯¸ì§€ ìƒì„± (auto_crop=True)
    logger.info(f"\nâ³ Step 3: Generating images via magic_book (this may take 10-15 minutes)...")
    logger.info(f"  â€¢ auto_crop=True (will generate 4 crops per image)")
    logger.info(f"  â€¢ save_locally=False (Supabase only)")

    results = generate_images_batch_and_save(
        prompts=midjourney_prompts,
        auto_crop=True,  # 4ê°œ í¬ë¡­ ìƒì„±
        save_locally=False,
        verbose=True
    )

    logger.success(f"\nâœ… Generated {len(results)} images")

    # 5. ì´ë¯¸ì§€ IDì™€ í‚¤ ë§¤í•‘
    key_to_image_id = {}
    for idx, (key, prompt) in enumerate(key_to_prompt.items()):
        if idx < len(results) and results[idx].success:
            # supabase_image_idsëŠ” [original_id, crop1_id, crop2_id, crop3_id, crop4_id] í˜•íƒœ
            image_id = results[idx].supabase_image_ids[0] if results[idx].supabase_image_ids else None
            if image_id:
                key_to_image_id[key] = image_id
                logger.success(f"  â€¢ {key} â†’ {image_id}")

    # 6. ê° ì•„í‹°í´ë³„ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì—…ë°ì´íŠ¸
    logger.info(f"\nğŸ“¥ Step 4: Downloading best crop images and updating articles...")

    public_images_dir = Path(__file__).parent.parent / 'public' / 'images'
    public_images_dir.mkdir(parents=True, exist_ok=True)

    for article_filename, image_mapping in ARTICLE_IMAGE_MAPPING.items():
        logger.info(f"\nğŸ—‚ï¸  Processing: {article_filename}")

        for img in image_mapping:
            key = img['key']
            filename = img['filename']

            if key not in key_to_image_id:
                logger.error(f"  âŒ No image generated for key: {key}")
                continue

            parent_image_id = key_to_image_id[key]

            try:
                # ìµœê³  í¬ë¡­ URL ì¡°íšŒ
                best_crop_url = get_best_crop_url(parent_image_id)
                logger.info(f"  â€¢ {key} â†’ {filename}")
                logger.info(f"    URL: {best_crop_url[:60]}...")

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                save_path = public_images_dir / filename
                download_image(best_crop_url, save_path)

            except Exception as e:
                logger.error(f"  âŒ Failed to process {filename}: {e}")

        # ë§ˆí¬ë‹¤ìš´ ì—…ë°ì´íŠ¸ (ì´ë¯¸ ê²½ë¡œê°€ ë§ìœ¼ë©´ ë³€ê²½ ì—†ìŒ)
        try:
            update_markdown_images(article_filename, image_mapping)
        except Exception as e:
            logger.error(f"  âŒ Failed to update markdown: {e}")

    logger.info("\n" + "=" * 80)
    logger.success("âœ… ALL DONE! ë¸”ë¡œê·¸ ì´ë¯¸ì§€ ì¬ìƒì„± ì™„ë£Œ")
    logger.info("=" * 80)
    logger.info(f"\nğŸ“Š Summary:")
    logger.info(f"  â€¢ Generated: {len(results)} Midjourney images (with 4 crops each)")
    logger.info(f"  â€¢ Downloaded: {len(ARTICLE_IMAGE_MAPPING) * 2} best crop images")
    logger.info(f"  â€¢ Updated: {len(ARTICLE_IMAGE_MAPPING)} blog articles")
    logger.info(f"\nğŸ“ Images saved to: {public_images_dir}")

if __name__ == "__main__":
    main()
