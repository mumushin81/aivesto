#!/usr/bin/env python3
"""
AIÂ·ì˜¤í”ˆì†ŒìŠ¤Â·í…Œí¬ íŠ¸ë Œë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°

ìˆ˜ì§‘ ëŒ€ìƒ:
- AI ë°˜ë„ì²´ ê¸°ìˆ  ë°œí‘œ
- ê¸°ì—… AI ì„œë¹„ìŠ¤ ì¶”ê°€/í™•ì¥
- ì„œë²„ GPU ìˆ˜ìš” ì¦ê°€ ë³´ê³ 
- ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ë°œí‘œ
- ë¹…í…Œí¬ í˜‘ë ¥/íŒŒíŠ¸ë„ˆì‹­

ë°ì´í„° ì†ŒìŠ¤:
- TechCrunch RSS
- The Verge RSS
- Reuters Tech
- Bloomberg Tech
- NVIDIA, OpenAI, Anthropic ê³µì‹ ë¸”ë¡œê·¸
"""

import os
import requests
import feedparser
from datetime import datetime
from dotenv import load_dotenv
from loguru import logger
from supabase import create_client
from typing import List, Dict, Any

load_dotenv()


class TechTrendsCollector:
    """AI/í…Œí¬ íŠ¸ë Œë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        self.supabase_url = os.getenv("SUPABASE_URL")
        self.supabase_key = os.getenv("SUPABASE_KEY")
        self.supabase = create_client(self.supabase_url, self.supabase_key)

        # RSS í”¼ë“œ URL
        self.rss_feeds = {
            "TechCrunch": "https://techcrunch.com/feed/",
            "TheVerge": "https://www.theverge.com/rss/index.xml",
            "Reuters_Tech": "https://www.reutersagency.com/feed/?taxonomy=best-topics&post_type=best",
            "Ars_Technica": "https://feeds.arstechnica.com/arstechnica/index"
        }

        # AI/í…Œí¬ ê´€ë ¨ í‚¤ì›Œë“œ
        self.ai_keywords = [
            "AI", "artificial intelligence", "GPU", "NVIDIA", "AMD",
            "ChatGPT", "OpenAI", "Anthropic", "Claude", "Gemini",
            "machine learning", "deep learning", "LLM", "transformer",
            "semiconductor", "chip", "server", "cloud computing",
            "partnership", "collaboration", "acquisition", "M&A"
        ]

        # ê´€ë ¨ ì¢…ëª©
        self.tech_stocks = {
            "AI_CHIP": ["NVDA", "AMD", "INTC"],
            "AI_SOFTWARE": ["MSFT", "GOOGL", "META"],
            "CLOUD": ["MSFT", "AMZN", "GOOGL"]
        }

    def collect_rss_news(self, feed_url: str, source_name: str) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        logger.info(f"ğŸ“Š {source_name} RSS ìˆ˜ì§‘ ì¤‘...")

        try:
            feed = feedparser.parse(feed_url)

            news_items = []
            for entry in feed.entries[:20]:  # ìµœê·¼ 20ê°œ
                # AI/í…Œí¬ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
                title = entry.get('title', '').lower()
                summary = entry.get('summary', '').lower()

                is_relevant = any(keyword.lower() in title or keyword.lower() in summary
                                for keyword in self.ai_keywords)

                if is_relevant:
                    news_items.append({
                        "source": source_name,
                        "title": entry.get('title', ''),
                        "link": entry.get('link', ''),
                        "published": entry.get('published', ''),
                        "summary": entry.get('summary', '')[:500]
                    })

            logger.success(f"  âœ… {source_name}: AI/í…Œí¬ ë‰´ìŠ¤ {len(news_items)}ê°œ ìˆ˜ì§‘")
            return news_items

        except Exception as e:
            logger.error(f"  âŒ {source_name} RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def collect_nvidia_blog(self) -> List[Dict[str, Any]]:
        """
        NVIDIA ê³µì‹ ë¸”ë¡œê·¸ì—ì„œ ê¸°ìˆ  ë°œí‘œ ìˆ˜ì§‘
        Source: NVIDIA Blog RSS or API
        """
        logger.info("ğŸ“Š NVIDIA ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘...")

        try:
            # NVIDIA Developer Blog RSS
            nvidia_rss = "https://blogs.nvidia.com/feed/"
            feed = feedparser.parse(nvidia_rss)

            nvidia_news = []
            for entry in feed.entries[:10]:
                nvidia_news.append({
                    "source": "NVIDIA Blog",
                    "title": entry.get('title', ''),
                    "link": entry.get('link', ''),
                    "published": entry.get('published', ''),
                    "category": "AI_CHIP"
                })

            logger.success(f"  âœ… NVIDIA ë¸”ë¡œê·¸: {len(nvidia_news)}ê°œ ìˆ˜ì§‘")
            return nvidia_news

        except Exception as e:
            logger.error(f"  âŒ NVIDIA ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def analyze_tech_trend_signal(self, news: Dict[str, Any]) -> Dict[str, Any]:
        """
        í…Œí¬ íŠ¸ë Œë“œ ë‰´ìŠ¤ì—ì„œ ì‹œê·¸ë„ ë¶„ì„

        ì‹œê·¸ë„ í¬ì¸íŠ¸:
        - "GPU shortage" â†’ NVDA ê°•ì„¸
        - "Microsoft AI monetization" â†’ MSFT ìƒìŠ¹
        - "China semiconductor ban" â†’ ë¯¸êµ­ AI ì¹© ê°•ì„¸
        """

        title = news.get("title", "").lower()
        summary = news.get("summary", "").lower()
        full_text = f"{title} {summary}"

        # ë‰´ìŠ¤ ì›ë³¸ ë°ì´í„° í¬í•¨
        signal = {
            "source": news.get("source"),
            "title": news.get("title"),
            "summary": news.get("summary", ""),  # ìš”ì•½ ì¶”ê°€
            "link": news.get("link", ""),  # ë§í¬ ì¶”ê°€
            "signal": None,
            "affected_stocks": [],
            "impact_score": 0,
            "trend_type": "GENERAL"  # ê¸°ë³¸ê°’
        }

        # GPU ê³µê¸‰ ë¶€ì¡± ì‹œê·¸ë„
        if "gpu shortage" in full_text or "gpu supply" in full_text:
            signal["signal"] = "NVDA_STRONG_BUY"
            signal["affected_stocks"] = ["NVDA"]
            signal["impact_score"] = 85
            signal["trend_type"] = "AI_CHIP"

        # Microsoft AI ì‹œê·¸ë„
        elif "microsoft" in full_text and ("ai" in full_text or "copilot" in full_text):
            signal["signal"] = "MSFT_AI_EXPANSION"
            signal["affected_stocks"] = ["MSFT"]
            signal["impact_score"] = 75
            signal["trend_type"] = "AI_SOFTWARE"

        # ì¤‘êµ­ ë°˜ë„ì²´ ì œì¬
        elif "china" in full_text and ("semiconductor" in full_text or "chip ban" in full_text):
            signal["signal"] = "US_AI_CHIP_RALLY"
            signal["affected_stocks"] = ["NVDA", "AMD", "INTC"]
            signal["impact_score"] = 80

        # OpenAI/Claude ì‹ ëª¨ë¸ ë°œí‘œ
        elif "gpt-5" in full_text or "claude" in full_text or "gemini" in full_text:
            signal["signal"] = "NEW_AI_MODEL_RELEASE"
            signal["affected_stocks"] = ["MSFT", "GOOGL", "NVDA"]
            signal["impact_score"] = 70

        # ë¹…í…Œí¬ íŒŒíŠ¸ë„ˆì‹­
        elif ("partnership" in full_text or "collaboration" in full_text) and \
             any(company in full_text for company in ["microsoft", "google", "amazon", "nvidia"]):
            signal["signal"] = "BIGTECH_PARTNERSHIP"
            signal["affected_stocks"] = self.extract_companies(full_text)
            signal["impact_score"] = 65

        return signal

    def extract_companies(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ"""
        companies = []
        company_map = {
            "nvidia": "NVDA",
            "microsoft": "MSFT",
            "google": "GOOGL",
            "amazon": "AMZN",
            "meta": "META",
            "apple": "AAPL",
            "amd": "AMD",
            "intel": "INTC"
        }

        for company_name, ticker in company_map.items():
            if company_name in text.lower():
                companies.append(ticker)

        return companies

    def save_to_database(self, data: List[Dict[str, Any]]):
        """ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥"""

        logger.info("ğŸ’¾ Supabaseì— ì €ì¥ ì¤‘...")

        for item in data:
            try:
                # ë‰´ìŠ¤ ì›ë³¸ ë°ì´í„° í¬í•¨í•˜ì—¬ ì €ì¥
                # summaryëŠ” titleê³¼ í•©ì³ì„œ ì €ì¥ (ìŠ¤í‚¤ë§ˆì— summary í•„ë“œ ì—†ìŒ)
                result = self.supabase.table('tech_trends').insert({
                    "trend_type": item.get("trend_type", "GENERAL"),
                    "title": item.get("title", "") + (f" - {item.get('summary', '')[:200]}" if item.get('summary') else ""),  # ì œëª© + ìš”ì•½
                    "source": item.get("source", ""),  # ë‰´ìŠ¤ ì¶œì²˜
                    "source_url": item.get("link", ""),  # ë‰´ìŠ¤ ë§í¬ (link â†’ source_url)
                    "companies": item.get("affected_stocks", []),
                    "signal": item.get("signal", ""),
                    "impact_score": item.get("impact_score", 0)
                }).execute()

                logger.success(f"  âœ… ì €ì¥ ì™„ë£Œ: {item.get('title', 'No title')[:50]}")

            except Exception as e:
                logger.error(f"  âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

    def run(self):
        """í…Œí¬ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤í–‰"""

        logger.info("=" * 60)
        logger.info("ğŸš€ AI/í…Œí¬ íŠ¸ë Œë“œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 60)

        all_news = []

        # 1. RSS í”¼ë“œ ìˆ˜ì§‘
        for source_name, feed_url in self.rss_feeds.items():
            news = self.collect_rss_news(feed_url, source_name)
            all_news.extend(news)

        # 2. NVIDIA ë¸”ë¡œê·¸
        nvidia_news = self.collect_nvidia_blog()
        all_news.extend(nvidia_news)

        # 3. ì‹œê·¸ë„ ë¶„ì„
        signals = []
        for news in all_news:
            signal = self.analyze_tech_trend_signal(news)
            if signal.get("signal"):
                signals.append(signal)
                logger.info(f"  ğŸ“ˆ ì‹œê·¸ë„: {signal['signal']} - {', '.join(signal['affected_stocks'])}")

        # 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        if signals:
            self.save_to_database(signals)

        logger.info("\n" + "=" * 60)
        logger.success(f"âœ… AI/í…Œí¬ ë‰´ìŠ¤ ìˆ˜ì§‘: {len(all_news)}ê°œ")
        logger.success(f"âœ… ì‹œê·¸ë„ ë°œê²¬: {len(signals)}ê°œ")
        logger.info("=" * 60)

        return {
            "news": all_news,
            "signals": signals
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    collector = TechTrendsCollector()
    result = collector.run()


if __name__ == "__main__":
    main()
