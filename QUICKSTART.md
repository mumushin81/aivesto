# ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ (Quick Start Guide)

íˆ¬ì ì‹œê·¸ë„ ê°ì§€ & ë¸”ë¡œê±° í†µí•© ì‹œìŠ¤í…œì„ 5ë¶„ ë§Œì— ì‹œì‘í•˜ì„¸ìš”!

---

## âœ… ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ ì„¤ì •
```bash
# Python 3.9 ì´ìƒ í•„ìš”
python --version

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install anthropic supabase loguru schedule flask flask-cors
```

### API í‚¤ ì¤€ë¹„
1. **Anthropic API Key** - https://console.anthropic.com
2. **Supabase URL & Key** - https://supabase.com
3. **Finnhub API Key** (ì„ íƒ) - https://finnhub.io
4. **Gmail App Password** (ì´ë©”ì¼ ì•Œë¦¼ìš©, ì„ íƒ)

---

## ğŸ“ Step 1: í™˜ê²½ ì„¤ì • (2ë¶„)

### íŒŒì¼ ìƒì„±: `.env`

```bash
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env íŒŒì¼ ìƒì„±
cat > .env << 'EOF'
# Supabase
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key

# APIs
ANTHROPIC_API_KEY=your_anthropic_key
FINNHUB_API_KEY=your_finnhub_key

# Email Alerts (ì„ íƒì‚¬í•­)
ALERT_RECIPIENTS=your_email@example.com
SENDER_EMAIL=noreply@aivesto.com
SENDER_PASSWORD=your_gmail_app_password

# ìˆ˜ì§‘ ê°„ê²© (ì´ˆ)
NEWS_COLLECTION_INTERVAL=900
ANALYSIS_INTERVAL=1800
ARTICLE_GENERATION_INTERVAL=3600

# ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜
MIN_RELEVANCE_SCORE=70
EOF
```

### í™˜ê²½ ë³€ìˆ˜ í™•ì¸
```bash
# .env íŒŒì¼ í™•ì¸
cat .env

# ë˜ëŠ” ì§ì ‘ ë‚´ë³´ë‚´ê¸°
export ANTHROPIC_API_KEY="your_key_here"
export SUPABASE_URL="your_url"
export SUPABASE_KEY="your_key"
```

---

## ğŸ”§ Step 2: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • (1ë¶„)

### Supabase í…Œì´ë¸” ìƒì„±

**1ë‹¨ê³„: Supabase ì½˜ì†”ì—ì„œ SQL ì‹¤í–‰**

```sql
-- news_raw í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS news_raw (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source TEXT,
    title TEXT,
    url TEXT UNIQUE,
    content TEXT,
    published_at TIMESTAMP,
    symbols TEXT[],
    metadata JSONB,
    created_at TIMESTAMP DEFAULT now()
);

-- analyzed_news í…Œì´ë¸” (SIGNAL LEVEL ì¶”ê°€)
CREATE TABLE IF NOT EXISTS analyzed_news (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    raw_news_id UUID REFERENCES news_raw(id),
    relevance_score INT,
    affected_symbols TEXT[],
    price_impact TEXT,
    importance TEXT,
    signal_level INT DEFAULT 4,
    analysis JSONB,
    created_at TIMESTAMP DEFAULT now()
);

-- published_articles í…Œì´ë¸”
CREATE TABLE IF NOT EXISTS published_articles (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title TEXT,
    content TEXT,
    analyzed_news_ids UUID[],
    wordpress_id INT,
    published_at TIMESTAMP,
    views INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT now()
);

-- ì¸ë±ìŠ¤ ìƒì„± (ì„±ëŠ¥ í–¥ìƒ)
CREATE INDEX idx_news_raw_created ON news_raw(created_at);
CREATE INDEX idx_analyzed_signal_level ON analyzed_news(signal_level);
CREATE INDEX idx_analyzed_relevance ON analyzed_news(relevance_score);
```

---

## ğŸ¯ Step 3: ì²« ì‹¤í–‰ (2ë¶„)

### ì˜µì…˜ 1: ì „ì²´ ì‹œìŠ¤í…œ í•œ ë²ˆ ì‹¤í–‰ (ê¶Œì¥)

```bash
# ëª¨ë“  ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
python main.py --mode once

# ì˜ˆìƒ ì¶œë ¥:
# === Running all jobs once ===
# === Starting news collection job ===
# === News collection completed: 20 items ===
# === Starting news analysis job (AUTO MODE) ===
# ğŸ¤– Starting automated analysis for 20 news items...
# âœ… Automated analysis completed: 15/20 items analyzed
# === Analysis completed: 15 items ===
# === Starting article generation job ===
# === Article generation completed: 2 articles (Tier: tier_1) ===
# === Starting cleanup job ===
# === Cleanup completed ===
```

### ì˜µì…˜ 2: ê°œë³„ ì‘ì—… ì‹¤í–‰

```bash
# ë‰´ìŠ¤ ìˆ˜ì§‘ë§Œ
python main.py --mode collect

# ë‰´ìŠ¤ ë¶„ì„ë§Œ (ìë™ ëª¨ë“œ - Claude API)
python main.py --mode analyze

# ë¸”ë¡œê·¸ ê¸€ ìƒì„± (Tier 1)
python main.py --mode generate --tier tier_1

# ë¸”ë¡œê·¸ ê¸€ ìƒì„± (Tier 2)
python main.py --mode generate --tier tier_2
```

### ì˜µì…˜ 3: ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì‹¤í–‰

```bash
# ë¬´í•œ ë£¨í”„ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ í™œì„±í™”)
python main.py --mode run

# Ctrl+Cë¡œ ì¤‘ë‹¨
# ë˜ëŠ” ë³„ë„ì˜ í„°ë¯¸ë„ì—ì„œ ê³„ì† ì‘ì—… ê°€ëŠ¥
```

---

## ğŸ“Š Step 4: ëŒ€ì‹œë³´ë“œ í™•ì¸ (ì¦‰ì‹œ)

### ëŒ€ì‹œë³´ë“œ ì„œë²„ ì‹œì‘

```bash
# ë³„ë„ í„°ë¯¸ë„ì—ì„œ
python dashboard/server.py

# ì¶œë ¥:
# * Running on http://0.0.0.0:5000
```

### ë¸Œë¼ìš°ì €ì—ì„œ ì ‘ì†

```
http://localhost:5000
```

**ëŒ€ì‹œë³´ë“œ í™”ë©´**:
- âœ… ì‹ í˜¸ ë ˆë²¨ë³„ í†µê³„ (Level 1-4)
- âœ… ê°€ê²© ì˜í–¥ ë¶„ì„ (ìƒìŠ¹/í•˜ë½/ì¤‘ë¦½)
- âœ… íŠ¸ë Œë”© ì¢…ëª© (ì‹ í˜¸ ê¸°ë°˜)
- âœ… ê¸´ê¸‰ ì‹ í˜¸ ë¦¬ìŠ¤íŠ¸
- âœ… ë†’ì€ ìš°ì„ ìˆœìœ„ ì‹ í˜¸
- âœ… ì‹¤ì‹œê°„ ìë™ ìƒˆë¡œê³ ì¹¨ (30ì´ˆ)

### API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

```bash
# ê¸´ê¸‰ ì‹ í˜¸ ì¡°íšŒ
curl http://localhost:5000/api/signals/urgent

# íŠ¸ë Œë”© ì¢…ëª© ì¡°íšŒ
curl http://localhost:5000/api/trending-symbols

# ëŒ€ì‹œë³´ë“œ ìš”ì•½
curl http://localhost:5000/api/dashboard

# ì‘ë‹µ í˜•ì‹ (JSON):
{
  "level": 1,
  "count": 5,
  "signals": [
    {
      "title": "Apple Q4 earnings beat expectations",
      "affected_symbols": ["AAPL"],
      "relevance_score": 95,
      "signal_level": 1
    }
  ]
}
```

---

## ğŸ“§ Step 5: ì´ë©”ì¼ ì•Œë¦¼ ì„¤ì • (ì„ íƒì‚¬í•­)

### Gmail ì•± ë¹„ë°€ë²ˆí˜¸ ìƒì„±

1. Google ê³„ì •: https://myaccount.google.com
2. ë³´ì•ˆ â†’ ì•± ë¹„ë°€ë²ˆí˜¸
3. "ë©”ì¼" + "Windows ì»´í“¨í„°" ì„ íƒ
4. ìƒì„±ëœ 16ìë¦¬ ë¹„ë°€ë²ˆí˜¸ ë³µì‚¬

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .envì— ì¶”ê°€
ALERT_RECIPIENTS=investor@example.com
SENDER_EMAIL=your_gmail@gmail.com
SENDER_PASSWORD=xxxx xxxx xxxx xxxx
```

### í…ŒìŠ¤íŠ¸ ì´ë©”ì¼ ë°œì†¡

```python
# test_email.py ì‹¤í–‰
from alerts import EmailAlertService

service = EmailAlertService()
service.send_daily_digest(['your_email@example.com'], hours=24)
print("âœ… í…ŒìŠ¤íŠ¸ ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ")
```

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

### 1. ë¸”ë¡œê±° ì¶”ì²œ í™•ì¸

```python
from blogger import ArticleQueueManager

queue = ArticleQueueManager()

# Tier 1 ì¶”ì²œ (ê¸´ê¸‰ ì‹ í˜¸)
recommendations = queue.get_recommended_signals(tier='tier_1', limit=20)
for rec in recommendations[:5]:
    print(f"ğŸ“ {rec['title'][:60]}")

# ìŠ¤ë§ˆíŠ¸ ì¶”ì²œ
smart = queue.get_smart_recommendations()
print(f"ì˜¤ëŠ˜ì˜ ì œì•ˆ: {len(smart['daily_suggestions'])} ì¢…ëª©")
```

### 2. ì‹ í˜¸ë³„ ë¸”ë¡œê·¸ ê¸€ ì‘ì„±

1. ëŒ€ì‹œë³´ë“œì—ì„œ "ê¸´ê¸‰ ì‹ í˜¸" í™•ì¸
2. `queue_manager.get_urgent_recommendations()` ë¡œ ì‹ í˜¸ ì¡°íšŒ
3. ì‹ í˜¸ ê¸°ë°˜ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ ì‘ì„±
4. ë°œí–‰ í›„ `mark_signal_published()` í˜¸ì¶œ

### 3. ëŒ€ì‹œë³´ë“œ ì»¤ìŠ¤í„°ë§ˆì´ì§•

**`dashboard/static/index.html`** ìˆ˜ì •:
- ìƒ‰ìƒ ë³€ê²½
- ìƒˆë¡œìš´ ì„¹ì…˜ ì¶”ê°€
- ì°¨íŠ¸ ì¶”ê°€

### 4. ìŠ¤ì¼€ì¤„ëŸ¬ ìµœì í™”

**`config/settings.py`** ìˆ˜ì •:
```python
# ë¶„ì„ ë°°ì¹˜ í¬ê¸° ì¦ê°€
NEWS_COLLECTION_INTERVAL = 900  # 15ë¶„
ANALYSIS_INTERVAL = 1800        # 30ë¶„
ARTICLE_GENERATION_INTERVAL = 3600  # 1ì‹œê°„
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### 1. "Anthropic API ì˜¤ë¥˜" ë˜ëŠ” "API í‚¤ ì˜ëª»ë¨"

```bash
# API í‚¤ í™•ì¸
echo $ANTHROPIC_API_KEY

# ì—†ìœ¼ë©´ ì„¤ì •
export ANTHROPIC_API_KEY="sk-ant-..."

# ë˜ëŠ” .env íŒŒì¼ í™•ì¸
cat .env | grep ANTHROPIC
```

### 2. "Supabase ì—°ê²° ì˜¤ë¥˜"

```bash
# URLê³¼ í‚¤ í™•ì¸
cat .env | grep SUPABASE

# Supabase ì½˜ì†”ì—ì„œ:
# 1. í”„ë¡œì íŠ¸ ì„¤ì • â†’ API
# 2. URLê³¼ Key (anon public) ë³µì‚¬
# 3. .env ì—…ë°ì´íŠ¸
```

### 3. "ë‰´ìŠ¤ê°€ ë¶„ì„ë˜ì§€ ì•ŠìŒ"

```bash
# ë¡œê·¸ í™•ì¸
tail -f logs/stock_news_*.log

# ì›ì¸:
# - ë‰´ìŠ¤ê°€ ìˆ˜ì§‘ë˜ì§€ ì•ŠìŒ: NEWS_COLLECTION_INTERVAL ì¤„ì´ê¸°
# - API í• ë‹¹ëŸ‰ ì´ˆê³¼: ê¸°ë‹¤ë¦¬ê¸°
# - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ì¸í„°ë„· ì—°ê²° í™•ì¸
```

### 4. "ëŒ€ì‹œë³´ë“œê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ"

```bash
# Flask ì„œë²„ í™•ì¸
ps aux | grep "dashboard/server.py"

# í¬íŠ¸ 5000 ì‚¬ìš© ì¤‘:
lsof -i :5000
kill -9 <PID>

# Flask ì¬ì‹œì‘
python dashboard/server.py
```

---

## ğŸ“Š ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸
tail -f logs/stock_news_*.log

# íŠ¹ì • ë‚ ì§œ ë¡œê·¸
ls -la logs/stock_news_2025-11-13.log

# ì—ëŸ¬ë§Œ í•„í„°ë§
grep ERROR logs/stock_news_*.log
```

### ë¶„ì„ í†µê³„

```python
# í˜„ì¬ ë¶„ì„ ìƒíƒœ í™•ì¸
from database.supabase_client import SupabaseClient

db = SupabaseClient()

# ë ˆë²¨ë³„ ì‹ í˜¸ ê°œìˆ˜
for level in [1, 2, 3, 4]:
    signals = db.get_signals_by_level(level, hours=24, limit=100)
    print(f"Level {level}: {len(signals)} signals")

# íŠ¸ë Œë”© ì¢…ëª©
trending = db.get_trending_symbols(hours=24, limit=10)
for s in trending:
    print(f"{s['symbol']}: {s['count']} signals")
```

---

## ğŸ¯ ì„±ê³µ í™•ì¸

ì‹œìŠ¤í…œì´ ì œëŒ€ë¡œ ì‘ë™í•˜ë©´:

âœ… **1ë‹¨ê³„**: ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ (20+ items)
âœ… **2ë‹¨ê³„**: ìë™ ë¶„ì„ ì™„ë£Œ (15+ analyzed)
âœ… **3ë‹¨ê³„**: ì‹ í˜¸ ë¶„ë¥˜ ì™„ë£Œ (Level 1-4 í‘œì‹œ)
âœ… **4ë‹¨ê³„**: ëŒ€ì‹œë³´ë“œ ë¡œë“œ (http://localhost:5000)
âœ… **5ë‹¨ê³„**: ê¸´ê¸‰ ì‹ í˜¸ í‘œì‹œ (ë¹¨ê°„ìƒ‰ ğŸ”´)

---

## ğŸ“ ì¶”ê°€ ë„ì›€

### ë¡œê·¸ ì½ê¸°

```
2025-11-13 10:30:45 | INFO | Relevance analyzer initialized with Claude API (automatic mode)
ğŸ¤– Starting automated analysis for 20 news items...
âœ… Automated analysis completed: 18/20 items analyzed
ğŸ”´ URGENT | 92 points | MSFT, NVDA
ğŸŸ  HIGH | 85 points | AAPL
```

### ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

**Q: ë¶„ì„ì´ ëŠë¦½ë‹ˆë‹¤**
A: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ ì¦ê°€
```python
# analyzers/analysis_pipeline.py
max_workers=10  # 5ì—ì„œ 10ìœ¼ë¡œ ì¦ê°€
```

**Q: ë¹„ìš©ì„ ì ˆê°í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤**
A: ë¶„ì„ ê°„ê²© ì¦ê°€
```python
ANALYSIS_INTERVAL = 3600  # 30ë¶„ì—ì„œ 1ì‹œê°„ìœ¼ë¡œ
```

**Q: íŠ¹ì • ì¢…ëª©ë§Œ ì¶”ì í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤**
A: config/settings.py ìˆ˜ì •
```python
TRACKED_SYMBOLS = ["MSFT", "AAPL", "NVDA"]  # ì›í•˜ëŠ” ì¢…ëª©ë§Œ
```

---

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

íˆ¬ì ì‹œê·¸ë„ ê°ì§€ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸš€

ë‹¤ìŒì€:
1. ëŒ€ì‹œë³´ë“œì—ì„œ ì‹ í˜¸ í™•ì¸
2. ë¸”ë¡œê±° ì¶”ì²œìœ¼ë¡œ ê¸€ì“°ê¸°
3. ì´ë©”ì¼ ì•Œë¦¼ ìˆ˜ì‹ 

í–‰ìš´ì„ ë¹•ë‹ˆë‹¤! ğŸ’°
